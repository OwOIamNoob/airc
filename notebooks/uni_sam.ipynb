{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from functools import  partial\n",
    "from queue import PriorityQueue, Queue\n",
    "import warnings\n",
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "import torch\n",
    "import sam2\n",
    "from sam2.build_sam import build_sam2, build_sam2_hf\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor as SAM\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import  Image\n",
    "import scipy\n",
    "import skimage\n",
    "\n",
    "import segmentationmetrics as segmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "gaussian = lambda x,mean,std: np.exp(-((x - mean) / std) ** 2 / 2) / (2.5 * std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(bound, pt, patch_size):\n",
    "    x = min(bound[1] - patch_size[1], max(0, pt[1] - patch_size[1] // 2))\n",
    "    y = min(bound[0] - patch_size[0], max(0, pt[0] - patch_size[0] // 2))\n",
    "    return x, y, patch_size[0], patch_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_pts(pts, bbox):\n",
    "    if isinstance(pts, list):\n",
    "        pts = np.array(pts)\n",
    "    if pts.shape[0] == 0:\n",
    "        return pts\n",
    "        \n",
    "    pts -= bbox[:2][::-1]\n",
    "    check = np.ones(pts.shape[0])\n",
    "    for i in range(pts.shape[1]):\n",
    "        check *= np.where((pts[:, i] > 0) & (pts[:, i] < bbox[i + 2]), 1, 0)\n",
    "    return pts[np.where(check > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompts(positive, negative):\n",
    "    pos_label = np.ones(positive.shape[0])\n",
    "    neg_label = np.zeros(negative.shape[0])\n",
    "    if negative.shape[0] == 0:\n",
    "        return positive[:, ::-1].copy(), pos_label\n",
    "    pts = np.concatenate([positive, negative], axis=0)\n",
    "    labels = np.concatenate((pos_label, neg_label))\n",
    "    return pts[:, ::-1].copy(), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for Sam inference\n",
    "class SamInferer:\n",
    "    def __init__(self, cfg = \"\", \n",
    "                    ckpt: str = \"\",\n",
    "                    model_id: str | None = None,\n",
    "                    patch_size = [256, 256],\n",
    "                    roi=[256, 256],\n",
    "                    root_area=500,\n",
    "                    max_roots=3,\n",
    "                    patience = 5,\n",
    "                    back_off = 5,\n",
    "                    alpha=0.1, \n",
    "                    d_alpha=0.2,\n",
    "                    beta=0.5, \n",
    "                    post_act=True, \n",
    "                    min_length=5,\n",
    "                    kernel_size=3,\n",
    "                    fill_kernel_size=5,\n",
    "                    neg_dis=15,\n",
    "                    pos_dis=15,\n",
    "                    pos_rad=256,\n",
    "                    pos_sc=1.,\n",
    "                    sampling_dis=3,\n",
    "                    neg_sampling_grid=6,\n",
    "                    thresh=0.75,\n",
    "                    decay=0.5,\n",
    "                    confidence=0.8,\n",
    "                    topk=2,\n",
    "                    stable_weight=3,\n",
    "                    gamma=1\n",
    "                    ):\n",
    "        if model_id is None:\n",
    "            self.predictor = SAM(build_sam2(cfg, ckpt))\n",
    "        else: \n",
    "            self.predictor = SAM(build_sam2_hf(model_id))\n",
    "        # Guidance and queries\n",
    "        self.sampling_dis = sampling_dis\n",
    "        self.marker = None\n",
    "        self.hist = None\n",
    "        self.patience = patience\n",
    "        self.stable_weight = stable_weight\n",
    "        self.back_off = 5\n",
    "        # self.queue = PriorityQueue()\n",
    "        self.queue = []\n",
    "\n",
    "        # Positive sampling\n",
    "        self.pos = np.zeros([0, 2], dtype=int)\n",
    "        self.pos_dis = pos_dis\n",
    "        self.pos_rad = pos_rad\n",
    "        self.pos_sc = pos_sc\n",
    "        # Negative sampling\n",
    "        self.neg = np.zeros([0, 2], dtype=int)\n",
    "        self.neg_dis = neg_dis\n",
    "        self.neg_sampling_grid = neg_sampling_grid\n",
    "        \n",
    "        # We gonna prioritize long flow over short ones\n",
    "        self.root = None\n",
    "\n",
    "        # Context related\n",
    "        self.step = 0\n",
    "        self.a_mask = None\n",
    "        self.b_mask = None\n",
    "        self.image = None \n",
    "        self.label = None\n",
    "        self.alpha = alpha\n",
    "        self.d_alpha = d_alpha\n",
    "        self.beta = float(beta)\n",
    "        self.confidence = confidence\n",
    "        self.decay = decay\n",
    "        self.post_act = post_act\n",
    "        self.weight = None\n",
    "        self.logits = None # Post-sigmoid or pre-sigmoid dependent\n",
    "        self.var = None\n",
    "        \n",
    "        # kernel configuration\n",
    "        self.patch_size = np.array(patch_size)\n",
    "        self.w_kernel = [cv2.getGaussianKernel(patch_size[0], roi[0]), cv2.getGaussianKernel(patch_size[1], roi[1])]\n",
    "        self.w_kernel = self.w_kernel[0] * self.w_kernel[1].T\n",
    "        self.w_kernel /= self.w_kernel.max() \n",
    "        # TO prevent vanishing\n",
    "        # self.w_kernel = np.interp(self.w_kernel, np.quantile(self.w_kernel, (0, 1)), (.2, 1))\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=2., tileGridSize=(51, 51))\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Uncertainty modelling\n",
    "        kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        fill_kernel_size = (fill_kernel_size, fill_kernel_size) if isinstance(fill_kernel_size, int) else fill_kernel_size\n",
    "        self.close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, kernel_size)\n",
    "        self.fill_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, fill_kernel_size)\n",
    "        self.stable_thresh = thresh\n",
    "        \n",
    "        # Flow skeleton\n",
    "        self.atlas = None\n",
    "        self.traits = None\n",
    "        self.dis_map = None\n",
    "        self.graph = defaultdict(list)\n",
    "        self.min_length = min_length\n",
    "        self.parent = dict()\n",
    "        self.roi = None\n",
    "        self.graph_root = np.zeros([0, 2], dtype=int)\n",
    "        self.root_area = root_area\n",
    "        self.max_roots = max_roots\n",
    "        self.topk = topk\n",
    "\n",
    "    # def pop(self):\n",
    "    #     if self.queue.qsize() == 0:\n",
    "    #         print(\"Empty queue !!!\")\n",
    "    #         return None\n",
    "    #     score, item = self.queue.get()\n",
    "    #     print(f\"Candidate of score {score}\")\n",
    "    #     return item\n",
    "    def pop(self):\n",
    "        if len(self.queue) == 0:\n",
    "            print(\"Empty queue !!!\")\n",
    "            return None\n",
    "        (score, item) = self.queue.pop(0)\n",
    "        print(f\"Candidate of score {score}\")\n",
    "        return item\n",
    "            \n",
    "    def valid_pts(self, pts):\n",
    "            pts -= self.root[None, :]\n",
    "            check = np.ones(pts.shape[0])\n",
    "            for i in range(pts.shape[1]):\n",
    "                check *= np.where((pts[:, i] > 0) & (pts[:, i] < self.patch_size[i]), 1, 0)\n",
    "            return pts[np.where(check > 0)]  \n",
    "        \n",
    "    def compose_prompts(self, pos, neg):\n",
    "        valid_pos = self.valid_pts(pos.copy())\n",
    "        pos_label = np.ones(valid_pos.shape[0])\n",
    "        # Generated locally so no need for projection\n",
    "        if self.neg.shape[0] > 0:\n",
    "            valid_neg = self.valid_pts(self.neg.copy())\n",
    "            valid_neg = np.concatenate([valid_neg, neg], axis=0)\n",
    "        else: \n",
    "            valid_neg = np.ones([0, 2])\n",
    "            \n",
    "        if valid_neg.shape[0] == 0:\n",
    "            return valid_pos[:, ::-1].copy(), pos_label\n",
    "        neg_label = np.zeros(valid_neg.shape[0])\n",
    "        # Must be in xy format\n",
    "        return np.concatenate([valid_pos, valid_neg], axis=0)[:, ::-1].copy(), np.concatenate([pos_label, neg_label], axis=0)\n",
    "\n",
    "    def set_marker(self, mask): \n",
    "        self.marker = smooth_mask(mask, 15, 5.)\n",
    "    \n",
    "    def read(self, image_path, channels=3):\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if img.shape[-1] > channels: \n",
    "            self.label = img[..., channels]\n",
    "        # Sharpen for better sense of boundary\n",
    "        # hsv_image = cv2.cvtColor(img[..., :channels], cv2.COLOR_BGR2HSV_FULL)\n",
    "        # unsharp = cv2.GaussianBlur(hsv_image, (3, 3), 0)\n",
    "        # hsv_image = 2 * hsv_image - unsharp\n",
    "        # self.image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB_FULL)\n",
    "        self.image = img[..., :channels][..., ::-1]\n",
    "        \n",
    "        self.a_mask = np.zeros(self.image.shape[:2], dtype=np.uint8)\n",
    "        self.b_mask = np.zeros(self.image.shape[:2], dtype=np.uint8)\n",
    "        self.hist = np.zeros(self.image.shape[:2])\n",
    "        self.weight = np.full(self.image.shape[:2], 1e-6)\n",
    "        self.logits = np.zeros(self.image.shape[:2]) if self.post_act else np.full(self.image.shape[:2], -10.) * self.weight\n",
    "        self.var = np.zeros_like(self.logits)\n",
    "        # self.beta = np.full(self.image.shape[:2], self.beta) * self.weight\n",
    "        self.output = np.zeros_like(self.a_mask)\n",
    "        self.box = self.image.shape[:2]\n",
    "\n",
    "        self.atlas = np.zeros_like(self.b_mask)\n",
    "        self.traits = np.zeros_like(self.b_mask)\n",
    "        self.dis_map = np.zeros([img.shape[0], img.shape[1], 2], dtype=float)\n",
    "    \n",
    "    # Generate negative prior\n",
    "    def negative_sampling(self, pos, debug = False):\n",
    "        # Prepare\n",
    "        pts = self.valid_pts(pos.copy())\n",
    "        dst = self.root + self.patch_size\n",
    "        a_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (self.neg_dis, self.neg_dis))\n",
    "        grad_kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "        \n",
    "        mask = self.b_mask[self.root[0]:dst[0], self.root[1]:dst[1]].copy()        \n",
    "        gradient = cv2.morphologyEx(cv2.dilate(mask, a_kernel, iterations=5), cv2.MORPH_GRADIENT, grad_kernel)\n",
    "\n",
    "        for pt in pts:\n",
    "            cv2.circle(gradient, pt[::-1], self.neg_dis * 3, 0, -1)\n",
    "\n",
    "        alpha_mask = scipy.ndimage.binary_fill_holes(self.a_mask[self.root[0]: dst[0], self.root[1]:dst[1]]).astype(int).astype(np.uint8)\n",
    "        alpha_mask = cv2.dilate(alpha_mask,  a_kernel, 3)\n",
    "\n",
    "        negative_field = gradient * (1 - alpha_mask)\n",
    "        # Discretize to 5 bin\n",
    "        output = grid_sampling(negative_field.astype(float), grid=self.neg_sampling_grid, alpha=0.01)\n",
    "        dis = np.triu(np.linalg.norm(output[None, :] - output[:, None, :], axis=2))\n",
    "        dis[dis == 0] = 1e5\n",
    "        drop = np.where(dis < self.neg_dis)[0]\n",
    "        accepted_neg = [i for i in range(output.shape[0]) if i not in drop]\n",
    "        output = output[accepted_neg]\n",
    "        \n",
    "        return {'pts': output} if not debug else {'pts': output, 'field': negative_field, 'b': alpha_mask, 'a': gradient}\n",
    "\n",
    "\n",
    "    def roi_(self, x, y):\n",
    "        if self.roi is None:\n",
    "            self.roi = np.array([y, x, y + self.patch_size[0], x + self.patch_size[1]])\n",
    "            return \n",
    "        self.roi[2] = max(self.roi[2], y + self.patch_size[0])\n",
    "        self.roi[0] = min(self.roi[0], y)\n",
    "        self.roi[3] = max(self.roi[3], x + self.patch_size[1])\n",
    "        self.roi[1] = min(self.roi[1], x)\n",
    "\n",
    "    def get_inbound(self, src, dst):\n",
    "        dis_map = self.dis_map[src[0]:dst[0], src[1]:dst[1]]\n",
    "        ske_map = np.linalg.norm(dis_map, axis=2)\n",
    "        mask = 1 - scipy.ndimage.binary_erosion(np.ones_like(ske_map), iterations=2)\n",
    "        pos = np.stack(np.where(mask * ske_map > 0)).T.astype(int)\n",
    "        if pos.shape[0] == 0:\n",
    "            return {'point': np.zeros([0, 2], dtype=int),\n",
    "                    'direction': np.zeros([0, 2], dtype=int)}\n",
    "        # direction = np.stack([dis_map[pt[0], pt[1]] for pt in pos], axis=0)\n",
    "        direction = dis_map[pos[:, 0], pos[:, 1]]\n",
    "        center_di = (dst - src) / 2 - pos\n",
    "        center_di /= np.linalg.norm(center_di, axis=1)[:, None]\n",
    "        # Get dominant axis\n",
    "        axis = np.argmax(np.abs(center_di / self.patch_size), axis=1)\n",
    "        # print(axis)\n",
    "        # print(np.stack([pos, center_di, direction], axis=1))\n",
    "        # Take dominant \n",
    "        valid = (np.take_along_axis(direction * center_di, axis[:, None], axis=1) >= 0).flatten()\n",
    "        # print(f\"Direction: {direction}, valid in {valid}\")\n",
    "        return {'point': pos[valid].copy().reshape(-1, 2),\n",
    "                'direction': direction[valid].copy().reshape(-1, 2)}\n",
    "\n",
    "    \n",
    "    def gather_prompts(self, src):\n",
    "        def filter(src, min_dis=50):\n",
    "            dis = np.linalg.norm(src[:, None, :] - src[None, :, :], axis=2)\n",
    "            valid = np.triu(np.ones_like(dis), k=1)\n",
    "            dis[valid==0] = 1e5\n",
    "            s = (dis < min_dis).sum(axis=0)\n",
    "            print(dis, s)\n",
    "            return src[s == 0]\n",
    "        res = []\n",
    "        index = 0\n",
    "        if len(self.queue) > 0:\n",
    "            while index < len(self.queue):\n",
    "                (score, item) = self.queue[index]\n",
    "                dis = np.abs(src - item)\n",
    "                pos = item - self.root\n",
    "                if (dis > self.pos_rad).any() or ((pos >= self.patch_size) | (pos < 0)).any() or self.hist[item[0], item[1]] < self.pos_sc: \n",
    "                    index += 1\n",
    "                else:\n",
    "                    res.append(self.queue.pop(index))\n",
    "            res = [src.tolist()] + [coord for s, coord in res]\n",
    "            res = filter(np.array(res), min_dis=self.pos_dis)\n",
    "            print(f\"Gathered {res.shape[0] - 1} prompts in single batch\")\n",
    "            if len(res) == 1:\n",
    "                return np.ones([0, 2])\n",
    "            return res[1:self.topk]\n",
    "        else:\n",
    "            return np.ones([0, 2])\n",
    "        \n",
    "    def infer(self, debug=False):\n",
    "        prompt = self.pop()\n",
    "        if prompt is None:\n",
    "            return {'ret': False}\n",
    "        if self.hist[prompt[0], prompt[1]] >= self.patience * 2:\n",
    "            print(f\"Point {prompt} got inferred {self.hist[prompt[0], prompt[1]]} times, skipping\")\n",
    "            return {'ret': False}\n",
    "        x, y, _, _ = get_bbox(self.box, prompt, self.patch_size)\n",
    "        self.roi_(x, y)\n",
    "        # Change base for referencing and post-processing.\n",
    "        self.root = np.array([y, x])\n",
    "        dst = self.root + self.patch_size\n",
    "        graph_root = self.graph_root - self.root\n",
    "        base_root = self.get_inbound(self.root, dst)\n",
    "        # print(base_root)\n",
    "        # Add graph root if it's present in the image\n",
    "        if (graph_root * (graph_root - self.patch_size)  <= 0).all(): \n",
    "            root_di = self.patch_size / 2 - graph_root\n",
    "            root_di /= (np.linalg.norm(root_di) + 1e-4)\n",
    "            print(\"Prompt presents in the patch\")\n",
    "            base_root['point'] =  np.concatenate([base_root['point'], graph_root], axis=0)\n",
    "            base_root['direction'] = np.concatenate([base_root['direction'], root_di], axis=0)\n",
    "        # Image in RGB format\n",
    "        patch = self.image[self.root[0]:dst[0], self.root[1]:dst[1]].copy()\n",
    "        self.predictor.set_image(patch)\n",
    "        # Positional Preparation\n",
    "        input_mask = self.b_mask[self.root[0]:dst[0], self.root[1]:dst[1]].copy()\n",
    "        # input_mask = 0\n",
    "        input_mask = np.maximum(input_mask, self.traits[self.root[0]:dst[0], self.root[1]:dst[1]] * (1 - scipy.ndimage.binary_fill_holes(input_mask)))\n",
    "        input_mask = cv2.resize(input_mask.astype(np.uint8), (256, 256))\n",
    "        pos = np.concatenate([np.array(prompt)[None, :], self.gather_prompts(prompt)], axis=0).astype(int)\n",
    "        print(\"Prompts:\", pos)\n",
    "        base_root['point'] = np.concatenate([base_root['point'], pos - self.root[None, :]], axis=0)\n",
    "        base_root['direction'] = np.concatenate([base_root['direction'], np.zeros([pos.shape[0], 2])], axis=0)\n",
    "        neg = self.negative_sampling(pos, debug=debug)\n",
    "        annotation, a_label = self.compose_prompts(pos, neg['pts'])\n",
    "        # Mind that mask input must halve the size, for size matching\n",
    "        print(input_mask.shape, a_label.shape, annotation.shape )\n",
    "        masks, scores, logits = self.predictor.predict(point_coords=annotation, \n",
    "                                                        point_labels=a_label, \n",
    "                                                        mask_input=input_mask[None, :] if input_mask.max() > 0 else None, \n",
    "                                                        multimask_output=False)\n",
    "        # cv2.resize(logits[0], self.patch_size, interpolation=cv2.INTER_LINEAR)\n",
    "        return {'ret': True,\n",
    "                'graph_root': base_root,\n",
    "                'input': patch,\n",
    "                'mask': masks[0], \n",
    "                'score': scores[0], \n",
    "                'logit': cv2.resize(cv2.GaussianBlur(logits[0], (3, 3), 0), self.patch_size, interpolation=cv2.INTER_LINEAR), \n",
    "                'pts': annotation, \n",
    "                'label': a_label,\n",
    "                'inp_mask': self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]].copy(),\n",
    "                'prompt': prompt,\n",
    "                'negative': neg}\n",
    "    \n",
    "    # Allow pipeline injection\n",
    "    def morphology_centers(self, segmentation, weight, minArea=2400, minW=3.):\n",
    "        # From an unknown respected lad\n",
    "        def get_center_of_mass(cnt):\n",
    "            M = cv2.moments(cnt)\n",
    "            cx = int(M['m10']/M['m00'])\n",
    "            cy = int(M['m01']/M['m00'])\n",
    "            return cy, cx\n",
    "        label_map = skimage.measure.label(segmentation, connectivity=2)\n",
    "        labels, counts = np.unique(label_map, return_counts=True)\n",
    "        # print(counts)\n",
    "        centers = []\n",
    "        out_seg = np.zeros_like(segmentation)\n",
    "        for label, count in zip(labels[1:int(min(len(labels), 1 + self.max_roots))], counts[1:int(min(len(labels), 1 + self.max_roots))]): \n",
    "            if count > minArea: \n",
    "                mask = (label_map == label).astype(np.uint8)\n",
    "                # Force the isle to be stable\n",
    "                if weight[mask == 1].mean() <= minW:\n",
    "                    continue\n",
    "                out_seg[mask > 0] = 1\n",
    "                cnt = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0][0]\n",
    "                centers.append((get_center_of_mass(cnt)))\n",
    "        return {'centers': np.array(centers) if len(centers) > 0 else np.zeros([0, 2]), \n",
    "                'mask': out_seg}\n",
    "\n",
    "    def graph_search(self, w_map, graph_root, mask, b_mask=None):\n",
    "        print(\"Searching\")\n",
    "        dst = self.root + self.patch_size\n",
    "        w_pts = np.array(np.where((w_map > 0) * (w_map < 5))).T\n",
    "        print(\"Skeleton size:\", w_pts.shape[0])\n",
    "        if w_pts.shape[0] > 4000:\n",
    "            return False, None, None\n",
    "        \n",
    "        dist = np.linalg.norm(w_pts - graph_root, axis=1)\n",
    "        nn = w_pts[np.argmin(dist)]\n",
    "        # Flow porting\n",
    "        # Update available mask\n",
    "        # ref_mask = self.b_mask[root[0]: dst[0], root[1]:dst[1]].copy()\n",
    "        tree = unilateral_dfs_tree( w_map.copy(), \n",
    "                                    mask, \n",
    "                                    tuple(nn), \n",
    "                                    b_mask=b_mask,\n",
    "                                    alpha=0.01, \n",
    "                                    thresh=0.8,\n",
    "                                    dis_map=self.dis_map[self.root[0]: dst[0], self.root[1]:dst[1]],\n",
    "                                    context_size=15,\n",
    "                                  )\n",
    "        # unilateral_dfs_tree(g_mask, inp_mask, start, weight = 1, alpha=0.1, thresh=0.95, context_size=3, dis_map=None)\n",
    "        # Ordering from leaves to roots\n",
    "        self.dis_map[self.root[0]: dst[0], self.root[1]:dst[1]] = (self.dis_map[self.root[0]: dst[0], self.root[1]:dst[1]] + scipy.ndimage.gaussian_filter(tree['dis_map'], [5., 5., 0])) / 2\n",
    "        print(\"Maximal outtro and intro:\", tree['status'].max())\n",
    "        self.traits[self.root[0]: dst[0], self.root[1]:dst[1]] = np.maximum(self.traits[self.root[0]: dst[0], self.root[1]:dst[1]], tree['status'] >= max(0, tree['status'].max() - 4))\n",
    "        cost_map = tree['cost']\n",
    "        branches = longest_path_branching(tree['dfs_tree'], tuple(nn), dead=tree['dead'])\n",
    "        valid_branches = [branches[i] for i in range(len(branches)) if len(branches[i]) >= self.min_length]\n",
    "        # Cost as normalized energy function along the geodesics\n",
    "        costs = [cost_map[branch[0][0], branch[0][1]] / len(branch) for branch in valid_branches]\n",
    "        return True, sorted(zip(costs, valid_branches), key=lambda x: x[0]), tree\n",
    "    \n",
    "    def iter(self, seg_res=None, debug=False):\n",
    "        self.step += 1\n",
    "        if seg_res is None:\n",
    "            seg_res = self.infer(debug=debug) \n",
    "        if seg_res['ret'] is False:\n",
    "            return seg_res\n",
    "        dst = self.root + self.patch_size\n",
    "        # Updating primitives\n",
    "        print(self.patch_size, seg_res['mask'].shape, seg_res['mask'].dtype)\n",
    "        # Never let it lower than alpha\n",
    "        score = max(seg_res['score'], self.alpha) \n",
    "        if score < self.alpha:\n",
    "            warnings.warn(f\"Model confidence {score} is hazardous, please make prompt to escape uncertainty\")\n",
    "        print(f\"Score {score}\")\n",
    "        print(f\"With {seg_res['label'].sum()} positives and {seg_res['label'].shape[0] - seg_res['label'].sum()} negatives\")\n",
    "        # Clean background from static gain\n",
    "        prob_map = sigmoid(seg_res['logit'])\n",
    "        # Model was trained to get threshold at 0.5 intuitively\n",
    "        if (prob_map >= self.alpha).sum() < self.root_area: \n",
    "            return {'ret': False}\n",
    "        sv = [0, \n",
    "              self.alpha / 2, \n",
    "              np.quantile(prob_map[prob_map >= self.alpha], 0.2), \n",
    "              np.quantile(prob_map[prob_map >= self.alpha], 0.5), \n",
    "              1]\n",
    "        dv = [0, 0, self.alpha * 0.8 + 0.2, self.alpha * 0.5 + 0.5, 1]\n",
    "        # print(sv, dv)\n",
    "        if self.post_act:\n",
    "            normed_map = scipy.ndimage.gaussian_filter(np.interp(prob_map, sv, dv), sigma=0.5) \n",
    "        else:\n",
    "            normed_map = np.where(np.abs(seg_res['logit']) > 10, np.sign(seg_res['logit']) * 10, seg_res['logit'])\n",
    "        normed_prob_map = normed_map if self.post_act is True else sigmoid(normed_map)\n",
    "        # print(f\"Quantile range: {sv}\")\n",
    "        # Weight ensemble\n",
    "        weight = score * self.w_kernel\n",
    "        # When weight is low, skip it to the next pred\n",
    "        ensemble_kernel = self.w_kernel / (self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] + self.w_kernel)\n",
    "        # Well, there is a case where confidence score is too low, so add tolerance \n",
    "        prev = self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] / self.weight[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        prev_prob = prev if self.post_act is True else np.interp(sigmoid(prev), sv, dv)\n",
    "        # prev_beta = self.beta[self.root[0]: dst[0], self.root[1]:dst[1]] / self.weight[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        prev_var = self.var[self.root[0]: dst[0], self.root[1]:dst[1]].copy()\n",
    "        # Update variance with momentum\n",
    "        if (normed_prob_map > self.beta).sum() <= self.root_area:\n",
    "            print(\"Segmentation mask is self contained, skipping\")\n",
    "            return {'ret': False, 'infer': seg_res, 'logit': normed_map, 'prob': normed_prob_map}\n",
    "        max_dev = 1 if self.post_act is True else min(5, max(2, -np.quantile(-seg_res['logit'], 0.0005) - np.quantile(seg_res['logit'], 0.05))) / 2\n",
    "        self.var[self.root[0]: dst[0], self.root[1]:dst[1]] = (1 - ensemble_kernel) * self.var[self.root[0]: dst[0], self.root[1]:dst[1]] + ensemble_kernel * np.minimum((normed_map - prev) ** 2, ( 0.3 * max_dev ) ** 2)\n",
    "        \n",
    "        # Subtract to get gain properties\n",
    "        true =  prev_prob > self.alpha\n",
    "        positive = normed_prob_map > self.alpha\n",
    "        # print(true.mean(), positive.mean())\n",
    "        roi = true | positive\n",
    "        dev = (self.var[self.root[0]: dst[0], self.root[1]:dst[1]] ** 0.5)[roi].mean()\n",
    "        liou = ((true & positive).sum() + 1) / (true.sum() + 1)\n",
    "        # print(dev.shape, beta.shape, prob_map.shape)\n",
    "        offset = (normed_map - prev) / (dev + 1e-3)\n",
    "        confidence = scipy.stats.norm.cdf(offset)\n",
    "        # confidence = np.where(offset < 0, 1 - confidence, confidence)\n",
    "        unstable = np.abs(confidence - 0.5) >= (self.confidence / 2) \n",
    "        stable = (prev_var < 0.05 * max_dev) & (self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] >= self.confidence * self.stable_weight) & roi\n",
    "        # If the overlap is too shallow, preserves the ensemble output\n",
    "        if liou < 0.5:\n",
    "            tn = true & ~positive \n",
    "            fp = ~true & positive\n",
    "            stable = stable | tn\n",
    "            unstable = unstable & fp\n",
    "        # Update properties\n",
    "        self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] += normed_map * weight\n",
    "        self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] += (-1 * stable + self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] / weight * unstable)  * weight * self.decay * (normed_map - prev)  \n",
    "        # Update decision boundary and weight\n",
    "        # gain = np.sign(normed_prob_map - prev_beta) * (np.abs(confidence - 0.5) - self.confidence / 2) * dev \n",
    "        # self.beta[self.root[0]: dst[0], self.root[1]:dst[1]] += (prev_beta) * weight  \n",
    "        self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] += weight\n",
    "        # Probabilistic Map\n",
    "        logit_map = self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] / self.weight[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        prob_map = np.interp(sigmoid(logit_map), sv, dv) if not self.post_act else logit_map\n",
    "        \n",
    "        # prob_map[unstable] = normed_prob_map[unstable]\n",
    "        # self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] = prob_map * self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] * self.decay + (1 - self.decay) * self.logits[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        # prob_map = prob_map ** 2\n",
    "        # Discretized Probabilistic Map \n",
    "        quantized_conf = np.interp(prob_map, (0, 1), (0, 255)).astype(np.uint8)\n",
    "        if not self.post_act: \n",
    "            prob_dev = np.abs(prob_map - prev_prob)[roi].mean()\n",
    "        else:\n",
    "            prob_dev = dev\n",
    "        prob_map = prob_map ** self.gamma\n",
    "        print(f\"Alpha adaptive threshold {- int(self.alpha * prob_dev * 255)}\")\n",
    "        # Discretized Probabilistic Map\n",
    "        # beta = self.beta[self.root[0]: dst[0], self.root[1]:dst[1]] / self.weight[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        # beta = (scipy.ndimage.gaussian_filter(beta, 2.) * 255).astype(np.uint8)\n",
    "        beta_mask = (quantized_conf >= int(self.beta * 255)).astype(np.uint8)\n",
    "        beta_mask = prune(morphology_closing(beta_mask, self.fill_kernel, 1), min_size=25)\n",
    "        beta_mask[~unstable] = np.maximum(self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]][~unstable], beta_mask[~unstable])\n",
    "        # self.b_mask[root[0]: dst[0], root[1]:dst[1]] = \n",
    "        # possible = (confidence >= self.alpha).astype(np.uint8)\n",
    "        # possible -= possible * self.b_mask[root[0]: dst[0], root[1]:dst[1]]\n",
    "        # possible = scipy.ndimage.binary_fill_holes(possible).astype(np.uint8)  * (1 - holes)\n",
    "        # Get possible\n",
    "        \n",
    "        discrete = cv2.adaptiveThreshold(quantized_conf, \n",
    "                                            1, \n",
    "                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, \n",
    "                                            91, \n",
    "                                            - max(15, int(self.confidence * prob_dev * 255)))\n",
    "        beta_mask = beta_mask + (1 - beta_mask) * discrete\n",
    "        holes = scipy.ndimage.binary_fill_holes(beta_mask) - beta_mask\n",
    "        # Filter out soft edge from original beta mask\n",
    "        possible = cv2.adaptiveThreshold(quantized_conf, \n",
    "                                            1, \n",
    "                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, \n",
    "                                            91, \n",
    "                                            - max(5, int(self.d_alpha * prob_dev * 255))) * (quantized_conf >= self.alpha * 255)\n",
    "        possible = possible + (1 - possible) * beta_mask\n",
    "        # Process \n",
    "        possible = prune(possible, min_size=100)\n",
    "        possible = scipy.ndimage.binary_closing(possible, iterations=2).astype(np.uint8)\n",
    "        possible = smooth_mask(possible, 5, 1.5) * (1 - holes)\n",
    "        \n",
    "        if self.marker is not None: \n",
    "            marker = self.marker[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "            possible *= marker\n",
    "            beta_mask *= marker\n",
    "        else:\n",
    "            marker = None\n",
    "        # marker = None\n",
    "        \n",
    "        possible = np.maximum(possible, beta_mask)\n",
    "        # possible = cv2.morphologyEx(possible, cv2.MORPH_CLOSE, self.fill_kernel, 2)\n",
    "        \n",
    "        # Stabilize skeleton\n",
    "        thin, thin_w = morphology_thinning(possible, return_weight=True)\n",
    "        print(\"Thin:\", possible.shape, thin_w.shape, thin.shape, possible.max())\n",
    "        thin_w = scipy.ndimage.grey_dilation(thin_w, size=5)\n",
    "        stable = scipy.ndimage.grey_closing(thin_w + self.atlas[self.root[0]: dst[0],self.root[1]:dst[1]], \n",
    "                                            size=10)\n",
    "        stable = prune(stable, min_size=10) * stable\n",
    "        \n",
    "        skeleton = skimage.morphology.skeletonize(stable)\n",
    "        stable = stable.astype(float)\n",
    "        stable = stable / (stable[stable > 0].mean() + 1e-3)\n",
    "        stable[stable == 0] = 1e2 \n",
    "        print(\"Stable:\", stable.shape, \"Skeleton values:\", np.unique(skeleton))\n",
    "        # skeleton = getLargestCC(skeleton).astype(int).astype(np.uint8)\n",
    "        # Flow map generation output['dist']\n",
    "        print(score, skeleton.shape, prob_map.shape)\n",
    "        w_map =  skeleton.astype(float) * ((1 - quantized_conf.astype(float) / 255) * (stable + 1e-1) + 1e-1) \n",
    "        \n",
    "        # w_map /= w_map.max()\n",
    "        w_map[self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]] * skeleton == 1] = np.maximum(w_map[self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]] * skeleton == 1], .1)\n",
    "        w_map[w_map > 5] = 5 \n",
    "        w_map /= 5\n",
    "        w_map[~skeleton] = 1e5\n",
    "        print(\"Weight map\",  w_map.max(), w_map.min())\n",
    "        # save_gray(\"/work/hpc/potato/airc/data/vis/debug.jpg\", w_map, 'viridis')\n",
    "        # Update root to center of mass\n",
    "        ref_mask = possible.copy()\n",
    "        acp_mask = np.zeros_like(ref_mask)\n",
    "        graph_root = seg_res['graph_root']\n",
    "        # print(f\"Graph roots: {graph_root['point']}\")\n",
    "        roots = []\n",
    "        direction = []\n",
    "        for i, root in enumerate(graph_root['point']):\n",
    "            # print(root)\n",
    "            if (acp_mask[root[0], root[1]] == 1) or (ref_mask[root[0], root[1]] == 0):\n",
    "                continue\n",
    "            roots.append(root)\n",
    "            try:\n",
    "                direction.append(graph_root['direction'][i]) \n",
    "            except:\n",
    "                print(f\"Graph mismatch {len(graph_root['point'])} and {len(graph_root['direction'])}\")\n",
    "            # print(direction)\n",
    "            temp = skimage.segmentation.flood(ref_mask, tuple(root.tolist()))\n",
    "            print(f\"Component at {root} coverage: {temp.mean()}\")\n",
    "            # ref_mask[temp > 0] = 0\n",
    "            acp_mask = np.maximum(acp_mask, temp)\n",
    "        # Flag all inferences\n",
    "        # self.hist[self.root[0]: dst[0], self.root[1]:dst[1]] += scipy.ndimage.binary_erosion(acp_mask, iterations=self.back_off - 1)\n",
    "        remains = possible * ( 1 - acp_mask )\n",
    "        if remains.max() > 0:\n",
    "            cc_centers = self.morphology_centers(remains, \n",
    "                                                 self.weight[self.root[0]: dst[0], self.root[1]:dst[1]], \n",
    "                                                 minArea=self.root_area)\n",
    "            print(\"CC max value:\", cc_centers['mask'].max())\n",
    "        else:\n",
    "            print(\"Cluster is empty\")\n",
    "            cc_centers = {'centers': np.zeros([0, 2]), 'mask': np.zeros([0, 2])}\n",
    "        roots = np.concatenate([np.array(roots).reshape(-1, 2), cc_centers['centers']], axis=0).astype(int)\n",
    "        direction = np.pad(np.array(direction).reshape(-1, 2), ((0, cc_centers['centers'].shape[0]), (0, 0)), mode='constant')\n",
    "        # import IPython; IPython.embed()\n",
    "        # Given the fact that the prompting\n",
    "        total_branches = []\n",
    "        result_mask = np.zeros_like(ref_mask)\n",
    "        canvas = np.zeros_like(thin_w)\n",
    "        for root in roots:\n",
    "            # Insert here\n",
    "            # print(root)\n",
    "            # Update history\n",
    "            if beta_mask[root[0], root[1]] == 1:\n",
    "                result_mask[skimage.segmentation.flood(beta_mask, tuple(root.tolist())) > 0] = 1\n",
    "            \n",
    "            ret, valid_branches, tree = self.graph_search(w_map, \n",
    "                                               root, \n",
    "                                               result_mask,\n",
    "                                                b_mask=beta_mask\n",
    "                                              )\n",
    "            if not ret: \n",
    "                continue\n",
    "            # valid_branches = [case[1] for case in valid_branches]\n",
    "            cost = tree['cost']\n",
    "            # state_map = scipy.ndimage.grey_dilation(tree['cost'], size=20)\n",
    "            total_branches += valid_branches\n",
    "            # valid_branches = sorted(valid_branches, key= lambda x: len(x))\n",
    "            for branch_idx, (cost, branch) in enumerate(valid_branches[:self.topk]):\n",
    "                # branch = smooth_graph(np.array(branch), 5, 1.5)\n",
    "                branch = np.array(branch)\n",
    "                i = 0\n",
    "                (x, y) = self.patch_size / 2\n",
    "                for i in range(self.back_off, len(branch)):\n",
    "                    x, y = branch[i]\n",
    "                    if confidence[x, y] > self.alpha:\n",
    "                        # if state_map[x, y] == 1 or state_map[x, y] % 2 == 0:\n",
    "                        break\n",
    "                        \n",
    "                for index, j in enumerate(range(i, max(i + 1, len(branch) - self.min_length), self.sampling_dis)):\n",
    "                    x, y = branch[j]\n",
    "                    score = prob_map[x, y] * (1000 - (len(branch) - j)) * cost * self.w_kernel[x, y]\n",
    "                    print(f\"Candidate {j} or length {len(branch)}: \", self.root + branch[j], score)\n",
    "                    self.add_queue(self.root + branch[j], prior = score)\n",
    "                \n",
    "                # Only get outgoing vertexes, \n",
    "                # if it loops into the main stream\n",
    "                # Then its a hole and already solved by fill holes\n",
    "                # if ref_mask[x, y] == 0: \n",
    "                #     # Roll back to prevent overflow\n",
    "                # else: \n",
    "                #     print(\"Point in mask already\")\n",
    "                # Draw on canvas for mask extraction later\n",
    "                for node in branch[i:]:\n",
    "                    # canvas[node[0], node[1]] = thin_w[node[0], node[1]]\n",
    "                    canvas[node[0], node[1]] = 1\n",
    "        \n",
    "        result_mask[((1 - scipy.ndimage.binary_fill_holes(beta_mask)) * canvas) > 0] = 1\n",
    "        \n",
    "        print(result_mask.max(), self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]].max(), beta_mask.max())\n",
    "        self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]] = np.maximum(self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]], result_mask)\n",
    "        # self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]][unstable] = result_mask[unstable].copy()\n",
    "        print(weight.max(), score, result_mask.max())\n",
    "        self.hist[self.root[0]: dst[0], self.root[1]:dst[1]] += cv2.dilate(result_mask, self.fill_kernel, iterations=2) * self.w_kernel * 2\n",
    "        self.atlas[self.root[0]: dst[0], self.root[1]:dst[1]] = np.maximum(self.atlas[self.root[0]: dst[0], self.root[1]:dst[1]], canvas)\n",
    "        metrics = eval(self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]].copy(), self.label[self.root[0]: dst[0], self.root[1]:dst[1]])\n",
    "\n",
    "        if debug:\n",
    "            return {'ret': True,\n",
    "                    'marker': marker,\n",
    "                    'roots': {'pts': roots, 'directions': direction},\n",
    "                    'infer': seg_res,\n",
    "                    'confidence': confidence,\n",
    "                    'beta': self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]].copy(),\n",
    "                    'src_beta': beta_mask,\n",
    "                    'prob_map': prob_map,\n",
    "                    'stable': stable,\n",
    "                    'thin': thin_w, \n",
    "                    'skeleton': w_map,\n",
    "                    'branches': total_branches,\n",
    "                    'possible': possible,\n",
    "                    'canvas': canvas,\n",
    "                    'metrics': metrics\n",
    "                    }\n",
    "        else: \n",
    "            return {'ret': True,\n",
    "                    'infer': seg_res,\n",
    "                    'branches': valid_branches,\n",
    "                    'metrics': metrics}\n",
    "\n",
    "    def update_graph(self, path):\n",
    "        # Inverse sampling from root to leaves\n",
    "        for i in range(len(path) - 1, 0, -1):\n",
    "            self.bi_add(tuple(self.root + path[i-1]), tuple(self.root + path[i]))\n",
    "\n",
    "    def bi_add(self, src, dst):\n",
    "        self.graph[src].append(dst)\n",
    "        self.graph[dst].append(src)\n",
    "\n",
    "    def check_hist(self, pt):\n",
    "        return self.hist[pt[0], pt[1]] > self.patience\n",
    "    \n",
    "    def add_queue(self, pt: list, prior: float = 1, isroot: bool =False):\n",
    "        if isroot:\n",
    "            # self.pos = np.concatenate([self.pos, np.array([pt])], axis=0)\n",
    "            # self.queue.put((prior, pt))\n",
    "            heappush(self.queue, (prior, pt))\n",
    "            self.graph_root = np.array(pt)[None, :]\n",
    "            return \n",
    "        \n",
    "        if self.check_hist(pt): \n",
    "            print(f\"Same position got infered for {self.hist[pt[0], pt[1]]} times, skipping {pt}\")\n",
    "            return\n",
    "            \n",
    "        # if pt[0] < 300:\n",
    "        #     print(f\"{pt} goes out of ROI\")\n",
    "        #     return\n",
    "            \n",
    "        score = max(1, int(prior + self.step))\n",
    "        entry = (score, pt)\n",
    "        try:\n",
    "            # self.queue.put(entry)\n",
    "            self.queue.append(entry)\n",
    "            self.queue = sorted(self.queue, key=lambda x: x[0])\n",
    "            print(self.queue)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bounding box Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute Intersection over Union (IoU) between two xyxy boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    # Compute intersection\n",
    "    xi1, yi1 = max(x1, x1_p), max(y1, y1_p)\n",
    "    xi2, yi2 = min(x2, x2_p), min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Compute union\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def compute_liou(box1, box2): \n",
    "    \"\"\"Compute Intersection over Union (IoU) between two xyxy boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    # Compute intersection\n",
    "    xi1, yi1 = max(x1, x1_p), max(y1, y1_p)\n",
    "    xi2, yi2 = min(x2, x2_p), min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Compute union\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    return inter_area / box1_area if box1_area > 0 else 0\n",
    "    \n",
    "def non_max_suppression(boxes, iou_threshold=0.5):\n",
    "    \"\"\"Prune overlapping bounding boxes using Non-Maximum Suppression (NMS).\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Sort boxes by area (largest first)\n",
    "    boxes = sorted(boxes, key=lambda b: (b[2] - b[0]) * (b[3] - b[1]), reverse=True)\n",
    "    selected_boxes = []\n",
    "\n",
    "    while boxes:\n",
    "        best_box = boxes.pop(0)\n",
    "        selected_boxes.append(best_box)\n",
    "        \n",
    "        boxes = [box for box in boxes if compute_iou(best_box, box) < iou_threshold]\n",
    "\n",
    "    return selected_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Output processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask(mask, kernel_size, sigma):\n",
    "    output = np.zeros_like(mask)\n",
    "    kernel = cv2.getGaussianKernel(kernel_size, sigma).squeeze()\n",
    "    cnts, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    refined = [np.stack([np.convolve(kernel, cnt[:, 0, i], mode='valid') for i in range(cnt.shape[-1])], axis=-1).round().astype(np.int32)[:, None, :] for cnt in cnts]\n",
    "    cv2.drawContours(output, refined, -1, 1, -1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_graph(cnt, kernel_size, sigma):\n",
    "    kernel = cv2.getGaussianKernel(kernel_size, sigma).squeeze()\n",
    "    return np.stack([np.convolve(kernel, cnt[:, i], mode='valid') for i in range(cnt.shape[-1])], axis=-1).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphology_closing(mask, kernel, iterations=1):\n",
    "    temp = cv2.dilate(mask, kernel, iterations)\n",
    "    return cv2.erode(temp, kernel, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    labels = skimage.measure.label(segmentation, connectivity=2)\n",
    "    assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "    largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(mask, min_size=5):\n",
    "    output = np.zeros(mask.shape, np.uint8)\n",
    "    _, label_im = cv2.connectedComponents(mask.astype(np.uint8), connectivity=8, ltype=cv2.CV_16U)\n",
    "    labels, counts = np.unique(label_im, return_counts=True)\n",
    "    for label, count in zip(labels[1:], counts[1:]):\n",
    "        im = label_im == label\n",
    "        if count >= min_size:\n",
    "            output = cv2.bitwise_or(output, im.astype(np.uint8))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphology_thinning(mask, return_weight=False):\n",
    "  #thinning word into a line\n",
    "  # Structuring Element\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n",
    "  close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "  weight = np.zeros(mask.shape, dtype=np.uint8)\n",
    "  # early stopping\n",
    "  if cv2.countNonZero(cv2.erode(mask,kernel)) == 0:\n",
    "    if return_weight: \n",
    "      return mask, weight\n",
    "    return mask\n",
    "\n",
    "  # Create an empty output image to hold values\n",
    "  thin = np.zeros(mask.shape,dtype='uint8')\n",
    "  # Loop until erosion leads to an empty set\n",
    "  while cv2.countNonZero(mask)!= 0:\n",
    "    # Erosion\n",
    "    erode = cv2.erode(mask,kernel)\n",
    "    # Opening on eroded image\n",
    "    opened = cv2.morphologyEx(erode,cv2.MORPH_OPEN, close_kernel)\n",
    "    # Subtract these two\n",
    "    subset = erode - opened\n",
    "    # Union of all previous sets\n",
    "    thin = cv2.bitwise_or(subset,thin)\n",
    "    # Keep the cummulative for weighting\n",
    "    weight += thin\n",
    "    # Set the eroded image for next iteration\n",
    "    mask = erode.copy()\n",
    "  \n",
    "  if not return_weight:\n",
    "    return thin\n",
    "  else:\n",
    "    return thin, weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_tree(mask, start):\n",
    "    rows, cols = mask.shape[:2]\n",
    "    stack = [start]\n",
    "    visited = set()\n",
    "    parent = {}\n",
    "    directions = [(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)] \n",
    "    dfs_tree = defaultdict(list)\n",
    "    \n",
    "    while stack:\n",
    "        x, y = stack.pop()\n",
    "\n",
    "        if (x, y) in visited:\n",
    "            continue\n",
    "        \n",
    "        visited.add((x, y))\n",
    "        \n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and mask[nx, ny] == 1 and (nx, ny) not in visited:\n",
    "                stack.append((nx, ny))\n",
    "                parent[(nx, ny)] = (x, y)\n",
    "                dfs_tree[(x, y)].append((nx, ny))\n",
    "    \n",
    "    return dfs_tree, parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_path(tree, start):\n",
    "    def dfs(node, path):\n",
    "        path.append(node)\n",
    "        max_path = path[:]\n",
    "        \n",
    "        for neighbor in tree[node]:\n",
    "            new_path = dfs(neighbor, path[:])\n",
    "            if len(new_path) > len(max_path):\n",
    "                max_path = new_path\n",
    "        \n",
    "        return max_path\n",
    "    \n",
    "    return dfs(start, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_field(logit_map, distance=15, beta=0.5, alpha=0.05):\n",
    "    if isinstance(distance, int):\n",
    "        distance = (distance, distance)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, distance)\n",
    "    dilated_mask = cv2.morphologyEx(cv2.dilate((logit_map > beta).astype(int).astype(np.uint8), kernel, iterations=5), cv2.MORPH_GRADIENT, kernel)\n",
    "    possible = scipy.ndimage.binary_fill_holes(np.where(logit_map > alpha, 1, 0)).astype(int).astype(np.uint8)\n",
    "    dilated_possible = cv2.dilate(possible, kernel, iterations=3)\n",
    "    negative_field = dilated_mask - dilated_mask * dilated_possible\n",
    "    return negative_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sampling(mask, grid=8, alpha=0.1):\n",
    "    if isinstance(grid, int):\n",
    "        grid = (grid, grid)\n",
    "    # We cannot sampling on grid\n",
    "    x, y = np.linspace(0, 1, grid[0])[:-1], np.linspace(0, 1, grid[1])[:-1]\n",
    "    patch_size = np.array(mask.shape[:2]) // grid\n",
    "    mesh = np.floor(np.stack(np.meshgrid(x, y), axis=-1).reshape(-1, 2) * mask.shape[:2]).astype(int)\n",
    "    def sample(src):\n",
    "        dst = src + patch_size\n",
    "        if np.mean(mask[src[0]:dst[0],src[1]:dst[1]]) < alpha:\n",
    "            return [-1, -1]\n",
    "        possible = np.array(np.where(mask[src[0]:dst[0],src[1]:dst[1]] > 0)).T\n",
    "        return possible[np.random.randint(0, high=possible.shape[0])] + src\n",
    "    output = np.apply_along_axis(sample, 1, mesh)\n",
    "    return output[output[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Geometry Graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = np.array([(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unilateral_dfs_tree(g_mask, inp_mask, start, b_mask = None, alpha=0.1, thresh=0.95, context_size=3, dis_map=None, tolerance=-0.5):\n",
    "    h, w = inp_mask.shape\n",
    "    global directions\n",
    "    rows, cols = g_mask.shape[:2]\n",
    "    stack = PriorityQueue()\n",
    "    # Dijikstra\n",
    "    status = np.zeros(g_mask.shape, dtype=int)\n",
    "\n",
    "    # This to indicate strictly incremental path\n",
    "    g_mask = np.abs(g_mask - 0.001)\n",
    "    cost = np.full_like(g_mask, 1e4)\n",
    "    px, py = start\n",
    "    if dis_map is not None:\n",
    "        stack.put((0, 0, (dis_map[px, py][None, :], start)))\n",
    "    else: \n",
    "        stack.put((0, 0, (np.zeros(1, 2), start)))\n",
    "    cost[px, py] = 0\n",
    "    leaves = set()\n",
    "    border = set()\n",
    "    begin = start\n",
    "    parent = {}\n",
    "    dead = set()\n",
    "    dfs_tree = defaultdict(list)\n",
    "    i = 0\n",
    "    while not stack.empty():\n",
    "        state, _, (moves, (x, y)) = stack.get()\n",
    "        # print(moves, x, y)\n",
    "        prior = np.mean(moves, axis=0)\n",
    "        dis_map[x, y] = (prior + dis_map[x, y]) / 2\n",
    "        # print(dis_map[x, y])\n",
    "        valid = (directions * dis_map[x, y]).sum(axis=1)\n",
    "        score = np.abs(valid).tolist()\n",
    "        indexes = [index for index, s in sorted(enumerate(score), key= lambda x: x[1]) if valid[index] >= tolerance]\n",
    "        accepted_dir = [directions[i] for i in indexes]\n",
    "        scores = [score[i] for i in indexes]\n",
    "        # print(accepted_dir)\n",
    "        for di in accepted_dir:\n",
    "            (dx, dy) = di.tolist()\n",
    "            if moves.shape[0] >= context_size: \n",
    "                moves = moves[1:].copy()\n",
    "            di_state = np.concatenate([moves, np.array([(dx, dy)])], axis=0)\n",
    "            \n",
    "            nx, ny = x + dx, y + dy\n",
    "            if ((nx - h) * nx >= 0) or ((ny - w) * ny >= 0):\n",
    "                continue\n",
    "            \n",
    "            if 0 <= nx < rows and 0 <= ny < cols and g_mask[nx, ny] > alpha and status[nx, ny] <= status[x, y]:\n",
    "                # Update on inverse confidence\n",
    "                if cost[nx, ny] > state + g_mask[nx, ny]:\n",
    "                    # Set cost as uncertainty gain \n",
    "                    i += 1\n",
    "                    cost[nx, ny] = state + g_mask[nx, ny]\n",
    "                    # print(cost[nx, ny])\n",
    "                    # Start by largest margin\n",
    "                    # Erase entry from other branch\n",
    "                    if (nx, ny) in parent.keys():\n",
    "                        # print(dfs_tree[parent[(nx, ny)]])\n",
    "                        (prx, pry) = parent[(nx, ny)]\n",
    "                        if cost[prx, pry] < cost[x, y]:\n",
    "                            dead.add(parent[(nx, ny)])\n",
    "                            dfs_tree[parent[(nx, ny)]].remove((nx, ny))\n",
    "                            stack.put((float(cost[nx, ny]), i, (np.zeros([1, 2]), (nx, ny))))\n",
    "                        else:\n",
    "                            stack.put((float(cost[nx, ny]), i, (di_state, (nx, ny))))\n",
    "                    else:\n",
    "                        stack.put((float(cost[nx, ny]), i, (di_state, (nx, ny))))\n",
    "                    parent[(nx, ny)] = (x, y)\n",
    "                    # Determine that they have gone out of beta mask\n",
    "                    # Odd for out-going, Even for in-going.\n",
    "                    if b_mask is None:\n",
    "                        if (g_mask[nx, ny] - thresh) * (g_mask[x, y] - thresh) < 0:\n",
    "                            if status[nx, ny] == 0:\n",
    "                                border.add((nx, ny))\n",
    "                            status[nx, ny] = status[x, y] + 1\n",
    "                        else:\n",
    "                            status[nx, ny] = status[x, y]\n",
    "                    else: \n",
    "                        if b_mask[nx, ny] != b_mask[x, y]:\n",
    "                            if status[nx, ny] == 0:\n",
    "                                border.add((nx, ny))\n",
    "                            status[nx, ny] = status[x, y] + 1\n",
    "                        else:\n",
    "                            status[nx, ny] = status[x, y]\n",
    "                        \n",
    "                    dfs_tree[(x, y)].append((nx, ny))\n",
    "            \n",
    "        if i == 0:\n",
    "            # Force leaves to be sink\n",
    "            status[x, y] += inp_mask[x, y] % 2\n",
    "            leaves.add((x, y))\n",
    "            # Continual of flow\n",
    "            if inp_mask[x, y] == 1:\n",
    "                begin = (x, y)\n",
    "    return {'dfs_tree': dfs_tree, \n",
    "            'parent': parent, \n",
    "            'cost': cost, \n",
    "            'border': border, \n",
    "            'leaves': leaves, \n",
    "            'status': status, \n",
    "            'begin': begin,\n",
    "            'dis_map': dis_map,\n",
    "            'dead': dead}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_tree(g_mask, inp_mask, start, weight = 1, alpha=0.1, thresh=0.95):\n",
    "    rows, cols = g_mask.shape[:2]\n",
    "    stack = PriorityQueue()\n",
    "    stack.put((0, start))\n",
    "\n",
    "    # Dijikstra\n",
    "    status = np.zeros(g_mask.shape, dtype=int)\n",
    "\n",
    "    # This to indicate strictly incremental path\n",
    "    g_mask = np.abs(g_mask - 0.001)\n",
    "    cost = np.full_like(g_mask, 1e4)\n",
    "    px, py = start\n",
    "    cost[px, py] = 0\n",
    "    leaves = set()\n",
    "    border = set()\n",
    "    begin = start\n",
    "    parent = {}\n",
    "    directions = [(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)] \n",
    "    dfs_tree = defaultdict(list)\n",
    "    \n",
    "    while not stack.empty():\n",
    "        \n",
    "        state, (x, y) = stack.get()\n",
    "        i = 0\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and g_mask[nx, ny] > alpha and status[nx, ny] <= status[x, y]:\n",
    "                # Update on inverse confidence\n",
    "                if cost[nx, ny] > state + (1 - g_mask[nx, ny]) * weight:\n",
    "                    # Set cost as uncertainty gain \n",
    "                    i += 1\n",
    "                    cost[nx, ny] = state + (1 - g_mask[nx, ny]) * weight\n",
    "                    # Start by largest margin\n",
    "                    stack.put((float(cost[nx, ny]), (nx, ny)))\n",
    "                    # Erase entry from other branch\n",
    "                    if (nx, ny) in parent.keys():\n",
    "                        # print(dfs_tree[parent[(nx, ny)]])\n",
    "                        dfs_tree[parent[(nx, ny)]].remove((nx, ny))\n",
    "\n",
    "                    parent[(nx, ny)] = (x, y)\n",
    "                    # Determine that they have gone out of mask\n",
    "                    # Odd for out-going, Even for in-going.\n",
    "                    if (g_mask[nx, ny] - thresh) * (g_mask[x, y] - thresh) < 0 :\n",
    "                        if status[nx, ny] == 0:\n",
    "                            border.add((nx, ny))\n",
    "                        status[nx, ny] = status[x, y] + 1\n",
    "                    else:\n",
    "                        status[nx, ny] = status[x, y] \n",
    "                    dfs_tree[(x, y)].append((nx, ny))\n",
    "            \n",
    "        if i == 0:\n",
    "            # Force leaves to be sink\n",
    "            status[x, y] += inp_mask[x, y] % 2\n",
    "            leaves.add((x, y))\n",
    "            # Continual of flow\n",
    "            if inp_mask[x, y] == 1:\n",
    "                begin = (x, y)\n",
    "    return {'dfs_tree': dfs_tree, \n",
    "            'parent': parent, \n",
    "            'cost': cost, \n",
    "            'border': border, \n",
    "            'leaves': leaves, \n",
    "            'status': status, \n",
    "            'begin': begin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do post \n",
    "def longest_path_branching(tree, start, dead = []):\n",
    "    visited = set()\n",
    "    branches = []\n",
    "    def dfs(node, path, visited, branches, state=True):\n",
    "        valid = False\n",
    "        if len(tree[node]) == 0:\n",
    "            if node in dead:\n",
    "                return [], False\n",
    "            return [node], True\n",
    "        if node in visited:\n",
    "            return path, False\n",
    "        paths = []\n",
    "        visited.add(node)\n",
    "        for neighbor in tree[node]:\n",
    "            if neighbor in visited:\n",
    "                continue\n",
    "            path, ret = dfs(neighbor, path[:], visited, branches)\n",
    "            if ret is True:\n",
    "                valid = True\n",
    "                paths.append(path + [node])\n",
    "\n",
    "        if len(paths) == 0:\n",
    "            max_path = [node]\n",
    "        else:\n",
    "            paths = sorted(paths, key= lambda x: -len(x))\n",
    "            max_path = paths[0]\n",
    "            branches += paths[1:]\n",
    "        return max_path, valid\n",
    "    output, _ = dfs(start, [], visited, branches)\n",
    "    return  branches + [output] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, label):\n",
    "    total = label.shape[0] * label.shape[1]\n",
    "    intersection = pred * label\n",
    "    int_count = intersection.sum()\n",
    "    union = cv2.bitwise_or(pred, label)\n",
    "    u_count = union.sum()\n",
    "    dice = 2 * int_count / (int_count + u_count + 1e-6)\n",
    "    iou = int_count / u_count \n",
    "    acc = (pred == label).sum() / total\n",
    "    recall = int_count / pred.sum()\n",
    "    f1 = 2 * acc * recall / (acc + recall)\n",
    "    return {'dice': dice,\n",
    "            'iou': iou,\n",
    "            'acc': acc,\n",
    "            'recall': recall,\n",
    "            'f1': f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "image_path = f\"../data/v2/{year}.png\"\n",
    "# /work/hpc/potato/airc/data/viz\n",
    "out_dir = f\"/work/hpc/potato/airc/data/vis_v2/{year}_free_v2\"\n",
    "template = \"{0}_v4.jpg\"\n",
    "model = \"sam2.1_hiera_s\"\n",
    "checkpoint = f\"/work/hpc/potato/sam/sam2/checkpoints/{model}.pt\"\n",
    "model_cfg = f\"configs/sam2.1/{model}.yaml\"\n",
    "model_id =  f\"facebook/{model}\"\n",
    "# y, x, h, w = 2400, 0, 1200, 1200\n",
    "# pts = np.array([570, 1660])\n",
    "first_pts = (565, 1655)\n",
    "# first_pts = (1220, 2260)\n",
    "# first_pts = (1565, 2213)\n",
    "patch_size = [512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"rm -rf {out_dir}\")\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    os.mkdir(f\"{out_dir}/strong\")\n",
    "    os.mkdir(f\"{out_dir}/weak\")\n",
    "    os.mkdir(f\"{out_dir}/logit\")\n",
    "    os.mkdir(f\"{out_dir}/output\")\n",
    "    os.mkdir(f\"{out_dir}/image\")\n",
    "    os.mkdir(f\"{out_dir}/thin\")\n",
    "    os.mkdir(f\"{out_dir}/skeleton\")\n",
    "    os.mkdir(f\"{out_dir}/ensembled_output\")\n",
    "    os.mkdir(f\"{out_dir}/confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A wrapper for Sam inference\n",
    "# class SamInferer:\n",
    "#     def __init__(self, cfg = \"\", \n",
    "#                     ckpt: str = \"\", \n",
    "#                     patch_size = [512, 512],\n",
    "#                     patience = 5,\n",
    "#                     min_dis = 10,\n",
    "#                     alpha=0.1, \n",
    "#                     beta=0.5, \n",
    "#                     post_act=True, \n",
    "#                     min_length=5,\n",
    "#                     kernel_size=3,\n",
    "#                     fill_kernel_size=7,\n",
    "#                     neg_dis=15,\n",
    "#                     sampling_grid=6,\n",
    "#                     thresh=0.75,\n",
    "#                     decay=0.5\n",
    "#                     ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gray(filename, image, cmap, invert=False, nonzero=True, output_size=None):\n",
    "    order = 1 if not invert else -1\n",
    "    if nonzero:\n",
    "        normed = image.copy()\n",
    "        masked = image[image != 0]\n",
    "        normed[normed != 0] = np.interp(masked, (masked.min(), masked.max()), (0, 1)[::order])\n",
    "    else:\n",
    "        normed = np.interp(image, (image.min(), image.max()), (0, 1)[::order])\n",
    "    rgb_image = mpl.colormaps[cmap](normed)\n",
    "    bgr_image = cv2.cvtColor((rgb_image * 255).astype(np.uint8), cv2.COLOR_RGB2BGR) \n",
    "    if output_size is not None:\n",
    "        bgr_image = cv2.resize(bgr_image, output_size)\n",
    "    print(cv2.imwrite(filename, bgr_image))\n",
    "\n",
    "def save_grey(filename, image, return_image=False):\n",
    "    normed = np.interp(image, (image.min(), image.max()), (0, 255)).astype(int).astype(np.uint8)\n",
    "    print(cv2.imwrite(filename, normed[..., None]))\n",
    "    if return_image:\n",
    "        return np.stack([normed, normed, normed], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'cfg': model_cfg, \n",
    "          'ckpt': checkpoint, \n",
    "          'roi': [256, 256],\n",
    "          'root_area': 800,\n",
    "          'max_roots': 2,\n",
    "         'patience': 3,\n",
    "         'beta': 0.65,\n",
    "         'd_alpha': 0.1,\n",
    "         'alpha': 0.25,\n",
    "         'decay': 0.7,\n",
    "         'patch_size': patch_size,\n",
    "         'thresh': 0.95,\n",
    "         'fill_kernel_size': 5,\n",
    "         'thresh': 0.95,\n",
    "         'min_length': 100, \n",
    "         'sampling_dis': 300, \n",
    "         'back_off': 15,\n",
    "         'neg_sampling_grid': 6,\n",
    "         'neg_dis': 15,\n",
    "         'pos_dis': 30,\n",
    "         'pos_rad': 200,\n",
    "         'pos_sc': 3.,\n",
    "         'confidence': 0.9,\n",
    "         'topk': 2,\n",
    "         'stable_weight': 1.5,\n",
    "         'post_act': False,\n",
    "         'gamma': 2.\n",
    "         }\n",
    "\n",
    "with open(f\"{out_dir}/config_{kwargs['beta']}_{kwargs['alpha']}.json\", \"w\") as file:\n",
    "    json.dump(kwargs, file, indent=4)\n",
    "\n",
    "# param = SamInferer( cfg=model_cfg, \n",
    "#                     ckpt=checkpoint,\n",
    "#                     roi=[288, 288],\n",
    "#                     root_area=800,\n",
    "#                     max_roots=2,\n",
    "#                     min_dis=15,\n",
    "#                     patience=4,\n",
    "#                     beta=0.6,\n",
    "#                     d_alpha=0.025,\n",
    "#                     alpha=0.2\n",
    "#                     decay=0.4,\n",
    "#                     patch_size=patch_size,\n",
    "#                     fill_kernel_size=5,\n",
    "#                     thresh=0.95,\n",
    "#                     min_length=100,\n",
    "#                     sampling_dis=300,\n",
    "#                     back_off=15,\n",
    "#                     neg_dis=20,\n",
    "#                     confidence=0.8,\n",
    "#                     topk=2,\n",
    "#                     stable_weight=0.8,\n",
    "#                     post_act=False,\n",
    "#                     gamma=1.5\n",
    "#                     )\n",
    "param = SamInferer(**kwargs)\n",
    "trials += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = {'dice': [], 'iou': [], 'acc': [], 'recall': [], 'f1': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.read(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param.set_marker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# param.add_queue(np.array([905, 2050]), isroot=True)\n",
    "param.add_queue(np.array(first_pts), isroot=True)\n",
    "param.neg = np.array([[275, 1100], [365, 1360], [900, 2050], [910, 2050], [5964, 3275]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### plt.imshow(skimage.morphology.skeletonize(output['possible']))\n",
    "for i in tqdm.tqdm(range(200), desc=\"Inference process\"):\n",
    "    if len(param.queue) == 0:\n",
    "        break\n",
    "    else:\n",
    "        print(param.queue)\n",
    "    iterations += 1\n",
    "    # print(f\"Iteration {iterations}\")\n",
    "    output = param.iter(debug=True)\n",
    "    print(param.roi)\n",
    "    if output['ret'] is True:\n",
    "        filename = f\"{year}_{iterations}_v4\"\n",
    "        src, dst = param.root, param.root + param.patch_size\n",
    "        # image = param.image[src[0]:dst[0], src[1]:dst[1], ::-1]\n",
    "        image = output['infer']['input']\n",
    "        \n",
    "        # logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "        logit = output['prob_map']\n",
    "        p_mask = skimage.morphology.skeletonize(scipy.ndimage.morphological_gradient(output['beta'], size=3)).astype(float)\n",
    "        a_mask = skimage.morphology.skeletonize(scipy.ndimage.morphological_gradient(output['possible'], size=3)).astype(float)\n",
    "        input_mask = skimage.morphology.skeletonize(scipy.ndimage.morphological_gradient(output['infer']['inp_mask'], size=3)).astype(float)\n",
    "        # p_mask = (p_mask >= np.quantile(p_mask[p_mask > 0], 0.5)).astype(np.uint8)\n",
    "        # Plotting\n",
    "        # plot = image * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "        plot = image / 255\n",
    "        plot = plot * (1 - p_mask[..., None]) + p_mask[..., None] * np.array([1, 0, 0])[None, None, :] + a_mask[..., None] * np.array([0, 1, 1])[None, None, :]\n",
    "        \n",
    "        input_image = image.copy().astype(float) / 255\n",
    "        input_image = input_image * (1 - input_mask[..., None]) + input_mask[..., None] * np.array([1, 1, 0])[None, None, :]\n",
    "        # if output['marker'] is not None:\n",
    "        #     mask = scipy.ndimage.morphological_gradient(output['marker'].astype(int), size=10)\n",
    "        #     mask = np.abs(mask)[..., None]\n",
    "        #     c_mask = mask * np.array([1, 1, 0])[None, None, :]\n",
    "        #     blend = mask * 0.2\n",
    "        #     plot = plot * (1 - blend) + blend * c_mask \n",
    "        # plot = plot.astype(np.uint8)\n",
    "        # plt.imshow(plot)\n",
    "        # plt.colorbar()\n",
    "        # \n",
    "        # cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(np.uint8))\n",
    "        branches = [np.array(branch) for (cost, branch) in output['branches']]\n",
    "        annotation = output['infer']['pts']\n",
    "        # print(annotation)\n",
    "        a_label = output['infer']['label'][:, None]\n",
    "        color = [0, 1, 0, 0.5] * a_label + [1, 0, 0, 0.5] * (1 - a_label)\n",
    "        # plt.scatter(annotation[:, 0], annotation[:, 1], s=50, c=color, marker='*')\n",
    "        # plot = image.astype(float) / 255\n",
    "        cv2.circle(plot, (np.array(output['infer']['prompt']) - src)[::-1], 5, [0.2, 1, 0, 0.5], -1)\n",
    "        \n",
    "        for pt, c in zip(annotation, color):\n",
    "            cv2.circle(plot, pt, 5, c, -1)\n",
    "            cv2.circle(input_image, pt, 5, c, -1)\n",
    "\n",
    "        # Save input prompt\n",
    "        cv2.imwrite(f\"{out_dir}/image/{filename}.jpg\", (input_image * 255)[..., ::-1].astype(np.uint8))\n",
    "        # plt.title(f\"Score: {output['infer']['score']}\")\n",
    "        # pt = output['infer']['prompt']\n",
    "        # plt.scatter(pt[1], pt[0])\n",
    "        cmap = plt.get_cmap('hsv')\n",
    "        for i, branch in enumerate(branches):\n",
    "            c_val = list(cmap(0.1 + 0.9 * (float(i) / len(branches))))\n",
    "            c_val[-1] = 0.3\n",
    "            \n",
    "            # rgb = (int(c_val[0] * 255), int(c_val[1] * 255), int(c_val[2] * 255))\n",
    "            cv2.polylines(plot, [branch[:, ::-1]], False, c_val, 1)\n",
    "            cv2.circle(plot, branch[0, ::-1], 10, c_val[:3], 1)\n",
    "            \n",
    "        print(output['roots'])\n",
    "        for i in range(output['roots']['pts'].shape[0]):\n",
    "            pt, di = output['roots']['pts'][i], output['roots']['directions'][i]\n",
    "            cv2.circle(plot, pt[::-1], 20, [0.5, 1, 0, 0.5], 1)\n",
    "            if np.linalg.norm(di.copy()) > 0:\n",
    "                # print(f\"Direction {di}\")\n",
    "                root_dst = (pt + di * 80).astype(int)\n",
    "                print(root_dst)\n",
    "                cv2.arrowedLine(plot, pt[::-1], root_dst[::-1], [0.5, 1, 0, 0.5], 2)\n",
    "            else: \n",
    "                print(f\"Graph root {pt} is stationary\")\n",
    "        # \"|\n",
    "        cv2.imwrite(f\"{out_dir}/output/{filename}.jpg\", (plot * 255)[..., ::-1])\n",
    "        # Writeout \n",
    "        skeleton = output['skeleton']\n",
    "        skeleton[skeleton > 5] = 0\n",
    "        \n",
    "        save_gray(f\"{out_dir}/logit/{filename}.jpg\", sigmoid(output['infer']['logit']), 'viridis', output_size=[256, 256])\n",
    "        save_gray(f\"{out_dir}/thin/{filename}.jpg\", output['thin'], 'viridis', nonzero=False, output_size=[256, 256])\n",
    "        save_gray(f\"{out_dir}/weak/{filename}.jpg\", output['possible'], 'viridis', nonzero=False, output_size=[256, 256])\n",
    "        save_gray(f\"{out_dir}/strong/{filename}.jpg\", output['beta'], 'viridis', nonzero=False, output_size=[256, 256])\n",
    "        save_gray(f\"{out_dir}/confidence/{filename}.jpg\", output['prob_map'], 'viridis', output_size=[256, 256])\n",
    "        save_gray(f\"{out_dir}/skeleton/{filename}.jpg\", output['skeleton'], 'viridis', invert=True, output_size=[256, 256])\n",
    "        \n",
    "        plt.imshow(cv2.resize(plot, [256, 256]))\n",
    "        plt.title(f\"Position: {output['infer']['prompt']}\\n Metrics: {output['metrics']}\")\n",
    "        plt.figure(figsize=(10,6))\n",
    "        if eval_res is None: \n",
    "            eval_res = dict(zip(output['metrics'].keys(), [[]] * len(output['metrics'])))\n",
    "        print(output['metrics'])\n",
    "        for key in output['metrics'].keys():\n",
    "                eval_res[key].append(float(output['metrics'][key]))\n",
    "    else:\n",
    "        print(\"Hehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param.roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{out_dir}/micro_{kwargs['beta']}_{kwargs['alpha']}.txt\", \"w\") as file:\n",
    "    file.write(f\"Number of trials: {trials}\\n\")\n",
    "    file.write(f\"Total iterations: {i}\\n\")\n",
    "    for key in eval_res.keys():\n",
    "        mean = np.mean(eval_res[key])\n",
    "        std = np.std(eval_res[key])\n",
    "        file.write(f\"{key} metrics: {mean} +- {std}\\n\")\n",
    "print(f\"Total iterations: {i}\\n\")\n",
    "print(f\"Total iterations: {i}\\n\")\n",
    "for key in eval_res.keys():\n",
    "    mean = np.mean(eval_res[key])\n",
    "    std = np.std(eval_res[key])\n",
    "    print(f\"{key} metrics: {mean} +- {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gray(f\"{out_dir}/weight.jpg\", param.weight, 'viridis', output_size=[450, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dst = [0, 0], param.box\n",
    "# src[0] = max(400, src[0])\n",
    "# weight = scipy.ndimage.gaussian_filter(param.weight, sigma=2.)\n",
    "logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight\n",
    "if not param.post_act:\n",
    "    prob = sigmoid(logit)\n",
    "else:\n",
    "    prob = logit\n",
    "# beta = param.beta / param.weight\n",
    "# logit = logit ** 2\n",
    "label = param.label[src[0]:dst[0], src[1]:dst[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{out_dir}/ensembled_output/{year}_logit_v0.jpg\")\n",
    "print(f\"{out_dir}/ensembled_output/{year}_logit_v2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grey(f\"{out_dir}/ensembled_output/{year}_logit_v2.jpg\", prob)\n",
    "save_grey(f\"{out_dir}/ensembled_output/{year}_mask_v2.jpg\", param.b_mask)\n",
    "# save_grey(f\"{out_dir}/ensembled_output/{year}_weak_v2.jpg\", param.a_mask[src[0]:dst[0], src[1]:dst[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi = param.roi\n",
    "# plt.imshow(logit[roi[0]:roi[2], roi[1]:roi[3]])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized = np.interp(prob ** 2, (0, 1), (0, 255)).astype(int).astype(np.uint8)\n",
    "clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(251, 251))\n",
    "quantized = clahe.apply(quantized)\n",
    "# save_grey(f\"{out_dir}/ensembled_output/{year}_beta_v2.jpg\", beta)\n",
    "b_mask = (quantized >= 150)\n",
    "b1 = cv2.adaptiveThreshold(quantized, \n",
    "                                    1, \n",
    "                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                    cv2.THRESH_BINARY, \n",
    "                                    91, \n",
    "                                    -15) * (quantized > 50)\n",
    "b2 = cv2.adaptiveThreshold(quantized, \n",
    "                                    1, \n",
    "                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                    cv2.THRESH_BINARY, \n",
    "                                    121, \n",
    "                                    -15) * (quantized > 100)\n",
    "mask = b_mask + (1 - b_mask) * np.maximum(b1, b2)\n",
    "mask = mask + (1 - mask) * param.b_mask[src[0]:dst[0], src[1]:dst[1]] * (quantized >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = cv2.erode(mask.astype(np.uint8), param.fill_kernel)\n",
    "mask = prune(mask, min_size=100)\n",
    "# mask = cv2.dilate(mask, param.fill_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros(mask.shape, np.uint8)\n",
    "_, label_im = cv2.connectedComponents(mask.astype(np.uint8), connectivity=8, ltype=cv2.CV_16U)\n",
    "labs, counts = np.unique(label_im, return_counts=True)\n",
    "for lab, count in zip(labs[1:], counts[1:]):\n",
    "    im = label_im == lab\n",
    "    if param.b_mask[src[0]:dst[0], src[1]:dst[1]][im].sum() <= 5:\n",
    "        continue \n",
    "    else: \n",
    "        output = np.maximum(output, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = prune(mask, min_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[:355] = 0\n",
    "mask[:355] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_grad = scipy.ndimage.morphological_gradient(label, size=3)\n",
    "label_grad = skimage.morphology.skeletonize(label_grad)\n",
    "mask_grad = scipy.ndimage.morphological_gradient(mask, size=3)\n",
    "mask_grad = skimage.morphology.skeletonize(mask_grad)\n",
    "mask_grad = scipy.ndimage.binary_fill_holes(mask) - mask + mask_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = param.image.copy()\n",
    "image = image * (1 - np.maximum(label_grad, mask_grad)[..., None]) + label_grad[..., None] * np.array((0, 255, 0))[None, None, :] + mask_grad[..., None] * np.array((255, 0, 0))[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(f\"{out_dir}/ensembled_output/{year}_filtered_v3.jpg\", mask[..., None] * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(f\"{out_dir}/ensembled_output/{year}_quantized_logit_{kwargs['beta']}_{kwargs['alpha']}.jpg\", quantized[..., None])\n",
    "cv2.imwrite(f\"{out_dir}/ensembled_output/{year}_annotated_{kwargs['beta']}_{kwargs['alpha']}.jpg\", image[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask *= scipy.ndimage.binary_dilation(param.label, iterations=80).astype(np.uint8)\n",
    "res = segmetrics.SegmentationMetrics(mask,label,(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{out_dir}/macro_{kwargs['beta']}_{kwargs['alpha']}.txt\", \"w\") as file:\n",
    "    file.write(str(res.get_df()))\n",
    "print(res.get_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "label = image[..., -1].copy()\n",
    "output = save_grey(\"dump.jpg\", mask, return_image=True)\n",
    "output[label == 0] *= np.array([0, 1, 0]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(f\"{out_dir}/output.jpg\", cv2.resize(output, (450, 600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# param.add_queue(np.array([1430, 2300]), isroot=True)\n",
    "# param.add_queue(np.array(first_pts), prior=100)\n",
    "# param.neg = np.array([[275, 1100], [365, 1360], [905, 2025], [900, 2050]])\n",
    "center = np.array([905, 2025])\n",
    "prompt = np.array([[1430, 2300], [565, 1655], [275, 1100], [365, 1360], [905, 2025], [900, 2050]])\n",
    "label = np.array([1, 1, 0, 0, 0, 0])\n",
    "color = [[0, 0, 255, 255], [0, 255, 0, 255]]\n",
    "dis = prompt - center\n",
    "print(dis)\n",
    "valid = (np.abs(dis) > 256).sum(axis=1)\n",
    "indexes = np.where(valid == 0)\n",
    "src, dst = center - 256, center + 256\n",
    "patch = param.image[src[0]:dst[0], src[1]:dst[1], ::-1].copy()\n",
    "pts, labs = (dis + 256)[indexes].copy().astype(int), label[indexes].copy().astype(int)\n",
    "for i in range(pts.shape[0]):\n",
    "    print(pts[i], labs[i])\n",
    "    cv2.circle(patch, pts[i][::-1], 10, color[labs[i]], -1)\n",
    "cv2.imwrite(f\"{out_dir}/prompt_neg_2.jpg\", cv2.resize(patch, (256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(param.w_kernel)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = output['infer']['negative']['field']\n",
    "# im = param.atlas[src[0]:dst[0], src[1]:dst[1]].copy()\n",
    "# w = param.weight[src[0]:dst[0], src[1]:dst[1]].copy()\n",
    "# im /= (w + 1e-3)\n",
    "# mask = param.var[src[0]:dst[0], src[1]:dst[1]]\n",
    "im = output['prob_map']\n",
    "# im = output['thin']\n",
    "# im = output['infer']['inp_mask']\n",
    "# im = sigmoid(output['infer']['logit']) ** 2\n",
    "# im = output['prob_map']  + output['beta']\n",
    "# mean = np.quantile(im[im > 0], .2)\n",
    "# print(mean)\n",
    "# im = output['beta']\n",
    "# im = output['hard']\n",
    "# im = scipy.ndimage.binary_fill_holes(im > param.alpha)\n",
    "# src = param.root\n",
    "# dst = src + param.patch_size\n",
    "# im = param.dis_map[src[0]:dst[0], src[1]:dst[1]]\n",
    "# print(im)\n",
    "# plt.imshow(np.linalg.norm(im, axis=-1))\n",
    "plt.imshow(im)\n",
    "plt.title(im.max())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst = param.roi[:2], param.roi[2:]\n",
    "# image = param.image[src[0]:dst[0], src[1]:dst[1]]\n",
    "# mask = param.label[src[0]:dst[0], src[1]:dst[1], None] \n",
    "# pred = param.b_mask[src[0]:dst[0], src[1]:dst[1], None]\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/image.jpg\", param.image[..., ::-1])\n",
    "# cv2.imwrite(\"/work/hpc/potato/airc/data/viz/seg_pred.jpg\", pred * 255)\n",
    "# cv2.imwrite(\"/work/hpc/potato/airc/data/viz/seg_label.jpg\", mask * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.threshold((logit * 255).astype(int).astype(np.uint8), 20, 1, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1])                                          `\n",
    "# plt.colorbar()\n",
    "gain = np.sqrt(np.pi)\n",
    "std = param.beta.mean() * param.w_kernel.mean() / scipy.ndimage.gaussian_filter(param.weight[src[0]:dst[0], src[1]:dst[1]], 30)\n",
    "dirac = lambda x, mean, std:  np.exp((x - mean) / (std * 1.44)) / (2.5 * std)\n",
    "beta = param.beta[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "\n",
    "# p_logit = skimage.restoration.denoise_wavelet(logit[..., None], channel_axis=-1, rescale_sigma=True)\n",
    "p_logit = output['prob_map']\n",
    "# softmask = dirac(logit ** 1.5, beta, std) * (logit > param.alpha)\n",
    "# grad = scipy.ndimage.gaussian_laplace(p_logit, sigma=1.)\n",
    "# grad = (grad / np.abs(grad).max())\n",
    "softmask = p_logit\n",
    "s = softmask ** 2\n",
    "# s = np.interp(softmask, (softmask.min(), softmask.max()), (0, 1))\n",
    "pseudo_mask = (s > .8).astype(np.uint8)\n",
    "cand = np.minimum(s * 255, 255).astype(np.uint8)\n",
    "\n",
    "# softmask = sigmoid(cand)\n",
    "\n",
    "# softmask[softmask > 255] = 255\n",
    "clahe = cv2.createCLAHE(clipLimit=2., tileGridSize=(31,31))\n",
    "cl1 = clahe.apply(cv2.GaussianBlur(cand, (3, 3), 0))\n",
    "mask = cv2.adaptiveThreshold(cl1, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 91, -15)\n",
    "pseudo_mask = (cl1 > 130).astype(np.uint8)\n",
    "# pseudo_mask = cv2.threshold(cl1, 200, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "# most_confidence =\n",
    "# mask = cv2.threshold(cand.astype(np.uint8), 200, 1, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# radius = 15\n",
    "# footprint = skimage.morphology.disk(radius)\n",
    "# local_otsu = skimage.filters.rank.otsu(softmask.astype(np.uint8), footprint)\n",
    "# mask = (softmask > local_otsu).astype(np.uint8)\n",
    "# mask = prune(mask, min_size=50)\n",
    "# mask = np.maximum(mask, pseudo_mask)\n",
    "# mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones([3, 3], dtype=np.uint8), 2)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=[8, 6])\n",
    "im = ax1.imshow(image)\n",
    "ax1.set_title(\"Original Logit\")\n",
    "fig.colorbar(im, ax=ax1)\n",
    "\n",
    "im=ax2.imshow(cl1)\n",
    "ax2.set_title(\"Softmask\")\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "im=ax3.imshow(pseudo_mask)\n",
    "ax3.set_title(\"Hard Thresholding\")\n",
    "fig.colorbar(im, ax=ax3)\n",
    "\n",
    "ax4.imshow(mask)\n",
    "ax4.set_title(\"Adaptive Thresholding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(softmask)\n",
    "pos = np.unravel_index(index, softmask.shape, order='F')\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = output['infer']['pts']\n",
    "plt.imshow(output['infer']['input'])\n",
    "plt.axis('off')\n",
    "# plt.scatter(pts[:, 0], pts[:, 1], marker='s', s=5, color=(1, 0, 1))\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['infer']['negative'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = param.root\n",
    "plt.imshow(param.image[x:x+512, y:y+512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts, _ = cv2.findContours(p_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "output = cv2.drawContours(image.copy(), cnts, -1, (0,0,255), 3)\n",
    "plt.imshow(output[..., ::-1])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = image[..., 3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred.shape, image.shape)\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/image.jpg\", image[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param.b_mask[src[0]:dst[0], src[1]:dst[1]] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_image = cv2.cvtColor(param.image, cv2.COLOR_BGR2HSV)\n",
    "unsharp = cv2.bilateralFilter(hsv_image,9,50,75)\n",
    "hsv_image = 2 * hsv_image - unsharp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = logit > 0.2\n",
    "# kernel = np.ones([3, 3], dtype=np.uint8)\n",
    "# skeleton = skimage.morphology.skeletonize(param.label)\n",
    "# plt.imshow(param.image[..., ::-1])\n",
    "# plt.imshow(image[..., ::-1])\n",
    "plt.imshow(hsv_image[src[0]:dst[0], src[1]:dst[1], 2])\n",
    "# keypoints = np.argmax(logit, axis=1)\n",
    "# print(keypoints.shape,  logit.shape, logit.max())\n",
    "# pts = [logit[i, pt] for i, pt in enumerate(keypoints)]\n",
    "# chosen = np.argmax(pts)\n",
    "# pt = keypoints[chosen]\n",
    "# print((chosen, pt) + src, logit[chosen, pt], src)\n",
    "# plt.colorbar()\n",
    "# plt.imshow(hsv_image[src[0]:dst[0], src[1]:dst[1], 0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.graph_root = (453, 1488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "plt.imshow(bgr_image[src[0]:dst[0], src[1]:dst[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = param.logits / param.weight\n",
    "param.b_mask = (logit ** 1.5 > param.beta).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = torch.from_numpy(param.a_mask[src[0]:dst[0], src[1]:dst[1]].copy()).float().cuda()\n",
    "x = torch.from_numpy(logit).float().cuda()\n",
    "canvas = torch.from_numpy(output['canvas']).float().cuda()\n",
    "canvas /= canvas.max()\n",
    "dist = geo.GSF2d(canvas[None, None, :], x[None, None, :], theta=1., v=8, lamb=.8, iter = 4).cpu().numpy()[0, 0]\n",
    "# plt.imshow(dist > np.quantile(dist[dist > 0], 0.2))\n",
    "plt.imshow(dist)\n",
    "plt.colorbar()\n",
    "plt.title(f\"{ np.quantile(dist[dist > dist.min()], 0.2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(skimage.morphology.skeletonize(output['possible']))\n",
    "\n",
    "src, dst = param.root, param.root + param.patch_size\n",
    "image = param.image[src[0]:dst[0], src[1]:dst[1]]\n",
    "logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "# Plotting\n",
    "plot = image[:, :, ::-1] * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "plot = plot * (1 - param.b_mask[src[0]:dst[0], src[1]:dst[1]][..., None])\n",
    "plt.imshow(plot)\n",
    "# plt.colorbar()\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(int).astype(np.uint8))\n",
    "branches = [np.array(branch) for branch in output['branches']]\n",
    "prompt = output['infer']['prompt'] - param.root\n",
    "annotation = output['infer']['pts']\n",
    "a_label = output['infer']['label'][:, None]\n",
    "color = [0, 0, 1, 0.5] * a_label + [1, 0, 0, 0.5] * (1 - a_label)\n",
    "plt.title(f\"Score: {output['infer']['score']}\")\n",
    "plt.scatter(annotation[:, 0], annotation[:, 1], s=50, c=color, marker='*')\n",
    "plt.scatter(prompt[1], prompt[0], s=100, c=[1, 1, 0, 1], marker='X')\n",
    "# pt = output['infer']['prompt']\n",
    "# plt.scatter(pt[1], pt[0])\n",
    "for branch in branches:\n",
    "    plt.plot(branch[:, 1], branch[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.98\n",
    "score = np.quantile(prob_map, p)\n",
    "prob_map = sigmoid(output['infer']['logit'])\n",
    "plt.imshow(prob_map > score)\n",
    "plt.title(f\"{p}-quantile {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = param.pos[:, None, :] - param.pos[None, :, :]\n",
    "loss = np.linalg.norm(dist, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit ** 2)\n",
    "plt.title(f\"Score {output['infer']['score']}\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit > 0.458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit > param.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logit.min(), logit.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = logit ** 0.3\n",
    "plt.imshow(image[:, :, ::-1] * (1- ret[..., None]) / 255 + ret[..., None])\n",
    "\n",
    "plt.scatter(pt[1], pt[0], s=0.5, c=(1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = param.iter()\n",
    "src, dst = param.root, param.root + param.patch_size\n",
    "image = param.image[src[0]:dst[0], src[1]:dst[1]]\n",
    "logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "# Plotting\n",
    "plot = image[:, :, ::-1] * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "plot = plot * (1 - param.b_mask[src[0]:dst[0], src[1]:dst[1]][..., None])\n",
    "plt.imshow(plot)\n",
    "# plt.colorbar()\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(int).astype(np.uint8))\n",
    "branches = [np.array(branch) for branch in output['branches']]\n",
    "pt = output['infer']['prompt'] - param.root\n",
    "plt.scatter(pt[1], pt[0])\n",
    "for branch in branches: \n",
    "    print(branch.shape[0])\n",
    "    plt.plot(branch[:, 1], branch[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)\n",
    "plt.title(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sigmoid(logit)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "possible = cv2.morphologyEx(np.where(prob > param.alpha, 1, 0).astype(int).astype(np.uint8), cv2.MORPH_CLOSE, kernel, 1)\n",
    "possible = scipy.ndimage.binary_fill_holes(possible)\n",
    "possible = getLargestCC(possible).astype(int).astype(np.uint8)\n",
    "plt.imshow(possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin, thin_w = morpholgy_thinning(possible, return_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thin_w * (thin_w < thin_w.max() * 0.75))\n",
    "plt.title(f\"Max:{thin_w.max()} \")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "stable = ((thin_w > 0) & (thin_w < thin_w.max() * 0.75)).astype(int).astype(np.uint8)\n",
    "closed = cv2.morphologyEx(stable, cv2.MORPH_DILATE, kernel, 1)\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", np.repeat(closed[:, :, None], 3, axis=-1) * 255)\n",
    "plt.imshow(closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = skimage.morphology.skeletonize(closed)\n",
    "plt.imshow(skeleton)\n",
    "main_branch = getLargestCC(skeleton).astype(int).astype(np.uint8)\n",
    "plt.imshow(main_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = (main_branch * prob) / score\n",
    "w_map[mask * main_branch == 1] = 1\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/output.jpg\", (np.repeat(w_map[:, :, None], 3, axis=2) * 255).astype(int).astype(np.uint8))\n",
    "plt.imshow(w_map)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit map is same shape to mask prompt, which is half the size of image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start at stable mask\n",
    "w_pts = np.array(np.where(w_map == 1)).T\n",
    "print(w_pts.shape)\n",
    "dist = np.linalg.norm(w_pts - res['pts'][-1] / 2, axis=1)\n",
    "nn = w_pts[np.argmin(dist)]\n",
    "print(nn, res['pts'][-1] / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_res = dfs_tree(w_map.copy(), res['inp_mask'], tuple(nn), alpha=0.01, thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, branches = longest_path_branching(dfs_res['dfs_tree'], tuple(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.array(path)\n",
    "branches = [np.array(branch) for branch in branches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = np.array(list(dfs_res['leaves']))\n",
    "cost_map = dfs_res['cost']\n",
    "cost_map[cost_map > 100] = 0\n",
    "plt.plot(path[:, 1], path[:, 0])\n",
    "for branch in branches: \n",
    "    plt.plot(branch[:, 1], branch[:, 0])\n",
    "plt.imshow(cost_map ** 0.3)\n",
    "plt.scatter(leaves[:, 1], leaves[:, 0], s=0.2)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_branch = [branches[i] for i in range(len(branches)) if len(branches[i]) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accepted_branch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
