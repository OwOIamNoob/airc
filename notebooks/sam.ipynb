{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from functools import  partial\n",
    "from queue import PriorityQueue, Queue\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import sam2\n",
    "from sam2.build_sam import build_sam2, build_sam2_hf\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor as SAM\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import  Image\n",
    "import scipy\n",
    "import skimage\n",
    "\n",
    "import FastGeodis as geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "gaussian = lambda x,mean,std: np.exp(-((x - mean) / std) ** 2 / 2) / (2.5 * std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(bound, pt, patch_size):\n",
    "    x = min(bound[1] - patch_size[1], max(0, pt[1] - patch_size[1] // 2))\n",
    "    y = min(bound[0] - patch_size[0], max(0, pt[0] - patch_size[0] // 2))\n",
    "    return x, y, patch_size[0], patch_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_pts(pts, bbox):\n",
    "    if isinstance(pts, list):\n",
    "        pts = np.array(pts)\n",
    "    if pts.shape[0] == 0:\n",
    "        return pts\n",
    "        \n",
    "    pts -= bbox[:2][::-1]\n",
    "    check = np.ones(pts.shape[0])\n",
    "    for i in range(pts.shape[1]):\n",
    "        check *= np.where((pts[:, i] > 0) & (pts[:, i] < bbox[i + 2]), 1, 0)\n",
    "    return pts[np.where(check > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompts(positive, negative):\n",
    "    pos_label = np.ones(positive.shape[0])\n",
    "    neg_label = np.zeros(negative.shape[0])\n",
    "    if negative.shape[0] == 0:\n",
    "        return positive[:, ::-1].copy(), pos_label\n",
    "    pts = np.concatenate([positive, negative], axis=0)\n",
    "    labels = np.concatenate((pos_label, neg_label))\n",
    "    return pts[:, ::-1].copy(), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for Sam inference\n",
    "class SamInferer:\n",
    "    def __init__(self, cfg = \"\", \n",
    "                    ckpt: str = \"\",\n",
    "                    model_id: str | None = None,\n",
    "                    patch_size = [512, 512],\n",
    "                    roi=[128, 128],\n",
    "                    root_area=500,\n",
    "                    max_roots=3,\n",
    "                    patience = 5,\n",
    "                    min_dis = 10,\n",
    "                    back_off = 5,\n",
    "                    alpha=0.1, \n",
    "                    beta=0.5, \n",
    "                    post_act=True, \n",
    "                    min_length=5,\n",
    "                    kernel_size=3,\n",
    "                    fill_kernel_size=7,\n",
    "                    neg_dis=15,\n",
    "                    pos_sampling_grid=3,\n",
    "                    neg_sampling_grid=6,\n",
    "                    thresh=0.75,\n",
    "                    decay=0.5,\n",
    "                    ):\n",
    "        if model_id is None:\n",
    "            self.predictor = SAM(build_sam2(cfg, ckpt))\n",
    "        else: \n",
    "            self.predictor = SAM(build_sam2_hf(model_id))\n",
    "        # Guidance and queries\n",
    "        self.pos = np.zeros([0, 2], dtype=int)\n",
    "        self.pos_sampling_grid = pos_sampling_grid\n",
    "        self.hist = np.zeros([0, 2], dtype=float)\n",
    "        self.patience = patience\n",
    "        self.min_dis = min_dis\n",
    "        self.back_off = 5\n",
    "        self.queue = PriorityQueue()\n",
    "\n",
    "        # Negative sampling\n",
    "        self.neg = np.zeros([0, 2], dtype=int)\n",
    "        self.neg_dis = neg_dis\n",
    "        self.neg_sampling_grid = neg_sampling_grid\n",
    "        # We gonna prioritize long flow over short ones\n",
    "        self.root = None\n",
    "\n",
    "        # Context related\n",
    "        self.step = 0\n",
    "        self.a_mask = None\n",
    "        self.b_mask = None\n",
    "        self.image = None \n",
    "        self.label = None\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.decay = decay\n",
    "        self.post_act = post_act\n",
    "        self.weight = None\n",
    "        self.logits = None # Post-sigmoid or pre-sigmoid dependent\n",
    "        self.var = None\n",
    "        # kernel configuration\n",
    "        self.patch_size = np.array(patch_size)\n",
    "        self.w_kernel = [cv2.getGaussianKernel(patch_size[0], roi[0]), cv2.getGaussianKernel(patch_size[1], roi[1])]\n",
    "        self.w_kernel = (self.w_kernel[0] / self.w_kernel[0][0, 0]) * (self.w_kernel[1] / self.w_kernel[1][0, 0]).T\n",
    "        self.w_kernel /= self.w_kernel.sum() \n",
    "        # TO prevent vanishing\n",
    "        self.w_kernel /= self.w_kernel.min()\n",
    "        # Uncertainty modelling\n",
    "        \n",
    "        kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        fill_kernel_size = (fill_kernel_size, fill_kernel_size) if isinstance(fill_kernel_size, int) else fill_kernel_size\n",
    "        self.close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, kernel_size)\n",
    "        self.fill_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, fill_kernel_size)\n",
    "        self.stable_thresh = thresh\n",
    "        \n",
    "        # Flow skeleton\n",
    "        self.atlas = None\n",
    "        self.graph = defaultdict(list)\n",
    "        self.min_length = min_length\n",
    "        self.parent = dict()\n",
    "        self.roi = None\n",
    "        self.graph_root = np.zeros([0, 2], dtype=int)\n",
    "        self.root_area = root_area\n",
    "        self.max_roots = max_roots\n",
    "\n",
    "    def pop(self):\n",
    "        if self.queue.qsize() == 0:\n",
    "            print(\"Empty queue !!!\")\n",
    "            return None\n",
    "        return self.queue.get()[1]\n",
    "        \n",
    "    def compose_prompts(self):\n",
    "        def valid_pts(pts):\n",
    "            pts -= self.root[None, :]\n",
    "            check = np.ones(pts.shape[0])\n",
    "            for i in range(pts.shape[1]):\n",
    "                check *= np.where((pts[:, i] > 0) & (pts[:, i] < self.patch_size[i]), 1, 0)\n",
    "            return pts[np.where(check > 0)]\n",
    "        \n",
    "        valid_pos = valid_pts(self.pos.copy())\n",
    "        pos_label = np.ones(valid_pos.shape[0])\n",
    "        if self.neg.shape[0] == 0:\n",
    "            return valid_pos[:, ::-1].copy(), pos_label\n",
    "        # Generated locally so no need for projection\n",
    "        valid_neg = self.neg\n",
    "        neg_label = np.zeros(valid_neg.shape[0])\n",
    "        # Must be in xy format\n",
    "        return np.concatenate([valid_pos, valid_neg], axis=0)[:, ::-1].copy(), np.concatenate([pos_label, neg_label], axis=0)\n",
    "\n",
    "    def read(self, image_path, channels=3):\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if img.shape[-1] > channels: \n",
    "            self.label = img[..., channels]\n",
    "        # Sharpen for better sense of boundary\n",
    "        # hsv_image = cv2.cvtColor(img[..., :channels], cv2.COLOR_BGR2HSV_FULL)\n",
    "        # unsharp = cv2.GaussianBlur(hsv_image, (3, 3), 0)\n",
    "        # hsv_image = 2 * hsv_image - unsharp\n",
    "        # self.image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB_FULL)\n",
    "        self.image = img[..., :channels][..., ::-1]\n",
    "        self.a_mask = np.zeros(self.image.shape[:2], dtype=np.uint8)\n",
    "        self.b_mask = np.zeros(self.image.shape[:2], dtype=np.uint8)\n",
    "        self.hist = np.zeros_like(self.b_mask)\n",
    "        self.logits = np.zeros(self.image.shape[:2])\n",
    "        self.var = np.zeros_like(self.logits)\n",
    "        self.weight = np.full(self.image.shape[:2], 1e-6)\n",
    "        self.beta = np.full(self.image.shape[:2], self.beta) * self.weight\n",
    "        self.output = np.zeros_like(self.a_mask)\n",
    "        self.box = self.image.shape[:2]\n",
    "\n",
    "        self.atlas = np.zeros_like(self.b_mask)\n",
    "    \n",
    "    # Generate negative prior\n",
    "    def negative_sampling(self, debug = False):\n",
    "        # Prepare\n",
    "        pts = (self.pos - self.root).round().astype(int)\n",
    "        dst = self.root + self.patch_size\n",
    "        a_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (self.neg_dis, self.neg_dis))\n",
    "        grad_kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "        \n",
    "        mask = self.b_mask[self.root[0]:dst[0], self.root[1]:dst[1]].copy()        \n",
    "        gradient = cv2.morphologyEx(cv2.dilate(mask, a_kernel, iterations=5), cv2.MORPH_GRADIENT, grad_kernel)\n",
    "        for pt in pts:\n",
    "            cv2.circle(gradient, pt[::-1], self.neg_dis * 3, 0, -1)\n",
    "\n",
    "        alpha_mask = scipy.ndimage.binary_fill_holes(self.a_mask[self.root[0]: dst[0], self.root[1]:dst[1]]).astype(int).astype(np.uint8)\n",
    "        alpha_mask = cv2.dilate(alpha_mask,  a_kernel, 3)\n",
    "\n",
    "        negative_field = gradient * (1 - alpha_mask)\n",
    "        # Discretize to 5 bin\n",
    "        output = grid_sampling(negative_field.astype(float), grid=self.neg_sampling_grid, alpha=0.01)\n",
    "        dis = np.triu(np.linalg.norm(output[None, :] - output[:, None, :], axis=2))\n",
    "        dis[dis == 0] = 1e5\n",
    "        drop = np.where(dis < self.neg_dis)[0]\n",
    "        accepted_neg = [i for i in range(output.shape[0]) if i not in drop]\n",
    "        output = output[accepted_neg]\n",
    "        \n",
    "        return {'pts': output} if not debug else {'pts': output, 'field': negative_field, 'b': alpha_mask, 'a': gradient}\n",
    "\n",
    "\n",
    "    def roi_(self, x, y):\n",
    "        if self.roi is None:\n",
    "            self.roi = np.array([y, x, y + self.patch_size[0], x + self.patch_size[1]])\n",
    "            return \n",
    "        self.roi[2] = max(self.roi[2], y + self.patch_size[0])\n",
    "        self.roi[0] = min(self.roi[0], y)\n",
    "        self.roi[3] = max(self.roi[3], x + self.patch_size[1])\n",
    "        self.roi[1] = min(self.roi[1], x)\n",
    "        \n",
    "    def infer(self, debug=False):\n",
    "        prompt = self.pop()\n",
    "        if prompt is None:\n",
    "            return {'ret': False}\n",
    "\n",
    "        # if self.check_hist(prompt): \n",
    "        #     print(f\"Same position got infered for too many times, Nothing changed\")\n",
    "        #     return {'ret': False}\n",
    "        \n",
    "        \n",
    "        x, y, _, _ = get_bbox(self.box, prompt, self.patch_size)\n",
    "        self.roi_(x, y)\n",
    "        # Change base for referencing and post-processing.\n",
    "        self.root = np.array([y, x])\n",
    "        dst = self.root + self.patch_size\n",
    "        \n",
    "        # Check for validity\n",
    "        \n",
    "        # if self.check_hist(prompt):\n",
    "        #     print(\"Same spot has been inferenced too many times, skipping\")\n",
    "        #     return {'ret': False}\n",
    "            \n",
    "        # Image in RGB format\n",
    "        patch = self.image[self.root[0]:dst[0], self.root[1]:dst[1]].copy()\n",
    "        self.predictor.set_image(patch)\n",
    "        # Negative sampling\n",
    "        neg = self.negative_sampling(debug=debug)\n",
    "        self.neg = neg['pts']\n",
    "        # Positive prompting\n",
    "        input_mask = cv2.resize(self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]], (256, 256))\n",
    "        self.pos = np.array(prompt)[None, :]\n",
    "        # self.pos = np.concatenate([self.graph_root, self.pos], axis=0)\n",
    "        \n",
    "        annotation, a_label = self.compose_prompts()\n",
    "        # print(annotation)\n",
    "        # a_mask = self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] / self.weight[self.root[0]: dst[0], self.root[1]:dst[1]]\n",
    "        # if not self.post_act:\n",
    "        #     a_mask = sigmoid(a_mask)\n",
    "        # # Quantile for recall\n",
    "        # # Always lower bound it for measure\n",
    "        # score = max(0.4, np.quantile(a_mask, 0.98))\n",
    "        # a_mask = np.where(a_mask > score, 1, 0).astype(np.float32)\n",
    "        \n",
    "        # a_mask = cv2.resize(a_mask, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        # print(a_mask.max())\n",
    "        # Mind that mask input must halve the size, for size matching\n",
    "        print(input_mask.shape, a_label.shape, annotation.shape )\n",
    "        masks, scores, logits = self.predictor.predict(point_coords=annotation, \n",
    "                                                        point_labels=a_label, \n",
    "                                                        mask_input=input_mask[None, :] if input_mask.max() > 0 else None, \n",
    "                                                        multimask_output=False)\n",
    "        # cv2.resize(logits[0], self.patch_size, interpolation=cv2.INTER_LINEAR)\n",
    "        return {'ret': True,\n",
    "                'input': patch,\n",
    "                'mask': masks[0], \n",
    "                'score': scores[0], \n",
    "                'logit': cv2.resize(logits[0], self.patch_size, interpolation=cv2.INTER_LINEAR), \n",
    "                'pts': annotation, \n",
    "                'label': a_label,\n",
    "                'inp_mask': self.b_mask[self.root[0]: dst[0], self.root[1]:dst[1]].copy(),\n",
    "                'prompt': prompt,\n",
    "                'negative': neg}\n",
    "    \n",
    "    # Allow pipeline injection\n",
    "    def morphology_centers(self, segmentation, weight, minArea=2400, minW=7.):\n",
    "        # From an unknown respected lad\n",
    "        def get_center_of_mass(cnt):\n",
    "            M = cv2.moments(cnt)\n",
    "            cx = int(M['m10']/M['m00'])\n",
    "            cy = int(M['m01']/M['m00'])\n",
    "            return cy, cx\n",
    "        label_map = skimage.measure.label(segmentation, connectivity=2)\n",
    "        labels, counts = np.unique(label_map, return_counts=True)\n",
    "        # print(counts)\n",
    "        centers = []\n",
    "        output_segmentation=\n",
    "        for label, count in zip(labels[1:min(len(labels), 1 + self.max_roots)], counts[1:min(len(labels), 1 + self.max_roots)]): \n",
    "            if count > minArea: \n",
    "                mask = (label_map == label).astype(np.uint8)\n",
    "                # Force the isle to be stable\n",
    "                if weight[mask == 1].mean() <= minW:\n",
    "                    continue\n",
    "                cnt = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0][0]\n",
    "                centers.append((get_center_of_mass(cnt)))\n",
    "        return {'centers': np.array(centers), \n",
    "                'mask': }\n",
    "    \n",
    "    def iter(self, seg_res=None, debug=False):\n",
    "        self.step += 1\n",
    "        if seg_res is None:\n",
    "            seg_res = self.infer(debug=debug) \n",
    "        if seg_res['ret'] is False:\n",
    "            return seg_res\n",
    "        dst = self.root + self.patch_size\n",
    "\n",
    "        # Updating primitives\n",
    "        print(self.patch_size, seg_res['mask'].shape, seg_res['mask'].dtype)\n",
    "        # Never let it lower than alpha\n",
    "        score = max(seg_res['score'], self.alpha) \n",
    "        if score < self.alpha:\n",
    "            warnings.warn(f\"Model confidence {score} is hazardous, please make prompt to escape uncertainty\")\n",
    "        print(f\"Score {score}\")\n",
    "        print(f\"With {seg_res['label'].sum()} positives and {seg_res['label'].shape[0] - seg_res['label'].sum()} negatives\")\n",
    "        # Clean background from static gain\n",
    "        prob_map = sigmoid(seg_res['logit']) \n",
    "        quantiles = [0.8, 1]\n",
    "        sv = [0, ]\n",
    "        dv = [0, 0, 1]\n",
    "        for val in quantiles:\n",
    "            sv.append(np.quantile(prob_map[prob_map > sv[-1]], val))\n",
    "        print(dict(zip(sv, dv)))\n",
    "        prob_map = np.interp(prob_map, sv, dv)\n",
    "\n",
    "        # Weight ensemble\n",
    "        weight = score * self.w_kernel\n",
    "        # When weight is low, skip it to the next pred\n",
    "        ensemble_kernel = self.w_kernel / self.w_kernel.max() * (1 - self.decay) * (self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] > 1)\n",
    "        # Well, there is a case where confidence score is too low, so add tolerance \n",
    "        self.beta[self.root[0]: dst[0], self.root[1]:dst[1]] += score * weight\n",
    "\n",
    "        # Update logits\n",
    "        self.var = ensemble_kernel * self.var[self.root[0]: dst[0], self.root[1]:dst[1]] + (1 - ensemble_kernel) * np.maximum((prob_map - self.logits[self.root[0]: dst[0], self.root[1]:dst[1]]) ** 2, 0.09)\n",
    "        \n",
    "        if self.post_act:\n",
    "            self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] += prob_map * weight\n",
    "        else: \n",
    "            self.logits[self.root[0]: dst[0], self.root[1]:dst[1]] += seg_res['logit'] * weight\n",
    "        self.weight[self.root[0]: dst[0], self.root[1]:dst[1]] += weight\n",
    "        \n",
    "        # Subtract to get gain properties\n",
    "        root = self.roi[:2]\n",
    "        dst = self.roi[2:]\n",
    "        beta = self.beta[root[0]: dst[0], root[1]:dst[1]] / self.weight[root[0]: dst[0], root[1]:dst[1]]\n",
    "        std = np.sqrt(self.var[root[0]: dst[0], root[1]:dst[1]])\n",
    "        prob_map = self.logits[root[0]: dst[0], root[1]:dst[1]] / self.weight[root[0]: dst[0], root[1]:dst[1]]\n",
    "        prob_map = sigmoid(prob_map) if not self.post_act else prob_map\n",
    "        confidence = 1 - gaussian(beta, prob_map, std)\n",
    "        # Get beta_mask\n",
    "\n",
    "        quantized_mask = np.round(np.power(prob_map, 1 - (prob_map - beta) / (1 - beta)) * 200)\n",
    "        quantized_mask[quantized_mask > 255] = 255\n",
    "        beta_mask = cv2.adaptiveThreshold(quantized_mask.astype(np.uint8), \n",
    "                                            1, \n",
    "                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, \n",
    "                                            151, \n",
    "                                            -10)\n",
    "        beta_mask = prune(beta_mask, min_size=25)\n",
    "        \n",
    "        self.b_mask[root[0]: dst[0], root[1]:dst[1]] = np.maximum(self.b_mask[root[0]: dst[0], root[1]:dst[1]], beta_mask) \n",
    "        \n",
    "        possible = (prob_map >= self.alpha).astype(np.uint8)\n",
    "        # possible -= possible * self.b_mask[root[0]: dst[0], root[1]:dst[1]]\n",
    "        possible = cv2.morphologyEx(possible, cv2.MORPH_OPEN, self.close_kernel, 2)\n",
    "        possible = scipy.ndimage.binary_fill_holes(possible).astype(np.uint8)\n",
    "        possible = prune(possible, min_size=50)\n",
    "        possible = smooth_mask(possible, 7, 3)\n",
    "        self.a_mask[root[0]: dst[0], root[1]:dst[1]] = cv2.bitwise_or(self.a_mask[root[0]: dst[0], root[1]:dst[1]], possible)\n",
    "        possible = np.maximum(possible, self.b_mask[root[0]: dst[0], root[1]:dst[1]])\n",
    "        possible = cv2.morphologyEx(possible, cv2.MORPH_CLOSE, self.fill_kernel, 2)\n",
    "        # Stabilize skeleton\n",
    "        thin, thin_w = morpholgy_thinning(possible, return_weight=True)\n",
    "        print(\"Thin:\", possible.shape, thin_w.shape, thin.shape, possible.max())\n",
    "        \n",
    "        stable = cv2.morphologyEx(thin_w + self.atlas[root[0]: dst[0], root[1]:dst[1]], cv2.MORPH_DILATE + cv2.MORPH_CLOSE, self.fill_kernel, 5)\n",
    "        stable = prune(stable, min_size=25)\n",
    "        # stable = smooth_mask(stable, 7, 3)\n",
    "        # stable = getLargestCC(stable).astype(int).astype(np.uint8)\n",
    "        # Skeletonize pruned mask\n",
    "        \n",
    "        print(\"Stable:\", stable.shape)\n",
    "        skeleton = skimage.morphology.skeletonize(stable)\n",
    "        # skeleton = getLargestCC(skeleton).astype(int).astype(np.uint8)\n",
    "        # Flow map generation output['dist']\n",
    "        print(score, skeleton.shape, prob_map.shape)\n",
    "        w_map = (skeleton * prob_map) / score\n",
    "        w_map[self.b_mask[root[0]: dst[0], root[1]:dst[1]] * skeleton == 1] = 1\n",
    "        w_map[w_map > 1] = 1 \n",
    "\n",
    "        # Update root to center of mass\n",
    "        self.graph_root = self.morphology_centers(self.b_mask[root[0]: dst[0], root[1]:dst[1]], minArea=self.root_area) + root[None, :]\n",
    "        \n",
    "        # import IPython; IPython.embed()\n",
    "        # Given the fact that the prompting \n",
    "        total_branches = []\n",
    "        canvas = np.zeros_like(thin_w)\n",
    "        for idx in range(self.graph_root.shape[0]):\n",
    "            w_pts = np.array(np.where(w_map == 1)).T\n",
    "            # print(w_pts.shape[0)\n",
    "            dist = np.linalg.norm(w_pts - (self.graph_root[idx] - root), axis=1)\n",
    "            nn = w_pts[np.argmin(dist)]\n",
    "            # Flow porting\n",
    "            # Update available mask\n",
    "            ref_mask = self.b_mask[root[0]: dst[0], root[1]:dst[1]].copy()\n",
    "            tree = dfs_tree(w_map.copy(), \n",
    "                            ref_mask, \n",
    "                            tuple(nn), \n",
    "                            alpha=0.01, \n",
    "                            thresh=0.8)\n",
    "            \n",
    "            # Ordering from leaves to roots\n",
    "            branches = longest_path_branching(tree['dfs_tree'], tuple(nn))\n",
    "            valid_branches = [branches[i] for i in range(len(branches)) if len(branches[i]) >= self.min_length]\n",
    "            total_branches += valid_branches\n",
    "            for branch in valid_branches:\n",
    "                x, y = branch[self.back_off]\n",
    "                # Only get outgoing vertexes, \n",
    "                # if it loops into the main stream\n",
    "                # Then its a hole and already solved by fill holes\n",
    "                # if ref_mask[x, y] == 0: \n",
    "                #     # Roll back to prevent overflow\n",
    "                self.add_queue(root + branch[self.back_off], prior = prob_map[x, y])\n",
    "                # else: \n",
    "                #     print(\"Point in mask already\")\n",
    "                # Draw on canvas for mask extraction later\n",
    "                for node in branch[3:]:\n",
    "                    # canvas[node[0], node[1]] = thin_w[node[0], node[1]]\n",
    "                    canvas[node[0], node[1]] = 1\n",
    "            \n",
    "            self.b_mask[root[0]: dst[0], root[1]:dst[1]] = np.maximum(self.b_mask[root[0]: dst[0], root[1]:dst[1]], canvas)\n",
    "            roi = (skimage.segmentation.flood_fill(self.b_mask[root[0]: dst[0], root[1]:dst[1]], \n",
    "                                                   tuple(nn), \n",
    "                                                   new_value=2,\n",
    "                                                   connectivity=2) == 2).astype(np.uint8)\n",
    "            self.hist[root[0]: dst[0], root[1]:dst[1]] += cv2.dilate(roi, self.fill_kernel, iterations=5).astype(np.uint8)\n",
    "        self.atlas[root[0]: dst[0], root[1]:dst[1]] = np.maximum(self.atlas[root[0]: dst[0], root[1]:dst[1]], canvas)\n",
    "        # Prepare for distance transform, the mask is from softmax, what can it possibly be ? \n",
    "        # image = torch.from_numpy(canvas).float().cuda()[None, None, :]\n",
    "        # mask = torch.from_numpy(prob_map).float().cuda()[None, None, :]\n",
    "        \n",
    "        # dist = geo.GSF2d(image, mask, theta=1., v=8, lamb=.8, iter = 4).cpu().numpy()[0, 0]\n",
    "        # dist[possible == 0] = dist.min()\n",
    "        # accepted_mask = (dist > np.quantile(dist[dist > dist.min()], 0.2)).astype(np.uint8)\n",
    "        # accepted_mask = getLargestCC(accepted_mask).astype(np.uint8)\n",
    "        metrics = eval(self.b_mask[root[0]: dst[0], root[1]:dst[1]].copy(), self.label[root[0]: dst[0], root[1]:dst[1]])\n",
    "        # del image\n",
    "        # del mask\n",
    "                # self.update_graph(branch)\n",
    "\n",
    "        if debug:\n",
    "            return {'ret': True,\n",
    "                    'infer': seg_res,\n",
    "                    'confidence': confidence,\n",
    "                    'beta': self.b_mask[root[0]: dst[0], root[1]:dst[1]].copy(),\n",
    "                    'prob_map': prob_map,\n",
    "                    'stable': stable,\n",
    "                    'thin': thin_w, \n",
    "                    'branches': total_branches,\n",
    "                    'possible': possible,\n",
    "                    'dist': dist,\n",
    "                    'canvas': canvas,\n",
    "                    'metrics': metrics\n",
    "                    }\n",
    "        else: \n",
    "            return {'ret': True,\n",
    "                    'infer': seg_res,\n",
    "                    'branches': valid_branches,\n",
    "                    'metrics': metrics}\n",
    "\n",
    "    def update_graph(self, path):\n",
    "        # Inverse sampling from root to leaves\n",
    "        for i in range(len(path) - 1, 0, -1):\n",
    "            self.bi_add(tuple(self.root + path[i-1]), tuple(self.root + path[i]))\n",
    "\n",
    "    def bi_add(self, src, dst):\n",
    "        self.graph[src].append(dst)\n",
    "        self.graph[dst].append(src)\n",
    "\n",
    "    def check_hist(self, pt):\n",
    "        return self.hist[pt[0], pt[1]] >= self.patience\n",
    "    \n",
    "    def add_queue(self, pt: list, prior: float = 1, isroot: bool =False):\n",
    "        if isroot:\n",
    "            # self.pos = np.concatenate([self.pos, np.array([pt])], axis=0)\n",
    "            self.queue.put((prior, pt))\n",
    "            self.graph_root = np.array(pt)[None, :]\n",
    "            return \n",
    "        \n",
    "        if self.check_hist(pt): \n",
    "            print(f\"Same position got infered for too many times, skipping {pt}\")\n",
    "            return\n",
    "            \n",
    "        if pt[0] < 400:\n",
    "            print(f\"{pt} goes out of ROI\")\n",
    "            return\n",
    "            \n",
    "        score = float(2 + prior + self.step * 0.1 + np.random.randn() * 0.01)\n",
    "        entry = (score, pt)\n",
    "        try:\n",
    "            self.queue.put(entry)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bounding box Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute Intersection over Union (IoU) between two xyxy boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    # Compute intersection\n",
    "    xi1, yi1 = max(x1, x1_p), max(y1, y1_p)\n",
    "    xi2, yi2 = min(x2, x2_p), min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Compute union\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def compute_liou(box1, box2): \n",
    "    \"\"\"Compute Intersection over Union (IoU) between two xyxy boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "\n",
    "    # Compute intersection\n",
    "    xi1, yi1 = max(x1, x1_p), max(y1, y1_p)\n",
    "    xi2, yi2 = min(x2, x2_p), min(y2, y2_p)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Compute union\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    return inter_area / box1_area if box1_area > 0 else 0\n",
    "    \n",
    "def non_max_suppression(boxes, iou_threshold=0.5):\n",
    "    \"\"\"Prune overlapping bounding boxes using Non-Maximum Suppression (NMS).\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Sort boxes by area (largest first)\n",
    "    boxes = sorted(boxes, key=lambda b: (b[2] - b[0]) * (b[3] - b[1]), reverse=True)\n",
    "    selected_boxes = []\n",
    "\n",
    "    while boxes:\n",
    "        best_box = boxes.pop(0)\n",
    "        selected_boxes.append(best_box)\n",
    "        \n",
    "        boxes = [box for box in boxes if compute_iou(best_box, box) < iou_threshold]\n",
    "\n",
    "    return selected_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Output processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask(mask, kernel_size, sigma):\n",
    "    output = np.zeros_like(mask)\n",
    "    kernel = cv2.getGaussianKernel(kernel_size, sigma).squeeze()\n",
    "    cnts, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    refined = [np.stack([np.convolve(kernel, cnt[:, 0, i], mode='valid') for i in range(cnt.shape[-1])], axis=-1).round().astype(np.int32)[:, None, :] for cnt in cnts]\n",
    "    cv2.drawContours(output, refined, -1, 1, -1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    labels = skimage.measure.label(segmentation, connectivity=2)\n",
    "    assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "    largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(mask, min_size=5):\n",
    "    output = np.zeros(mask.shape, np.uint8)\n",
    "    _, label_im = cv2.connectedComponents(mask.astype(np.uint8), connectivity=8, ltype=cv2.CV_16U)\n",
    "    labels, counts = np.unique(label_im, return_counts=True)\n",
    "    for label, count in zip(labels[1:], counts[1:]):\n",
    "        im = label_im == label\n",
    "        if count >= min_size:\n",
    "            output = cv2.bitwise_or(output, im.astype(np.uint8))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpholgy_thinning(mask, return_weight=False):\n",
    "  #thinning word into a line\n",
    "  # Structuring Element\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n",
    "  close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "  weight = np.zeros(mask.shape, dtype=np.uint8)\n",
    "  # early stopping\n",
    "  if cv2.countNonZero(cv2.erode(mask,kernel)) == 0:\n",
    "    if return_weight: \n",
    "      return mask, weight\n",
    "    return mask\n",
    "\n",
    "  # Create an empty output image to hold values\n",
    "  thin = np.zeros(mask.shape,dtype='uint8')\n",
    "  # Loop until erosion leads to an empty set\n",
    "  while cv2.countNonZero(mask)!= 0:\n",
    "    # Erosion\n",
    "    erode = cv2.erode(mask,kernel)\n",
    "    # Opening on eroded image\n",
    "    opened = cv2.morphologyEx(erode,cv2.MORPH_OPEN,close_kernel)\n",
    "    # Subtract these two\n",
    "    subset = erode - opened\n",
    "    # Union of all previous sets\n",
    "    thin = cv2.bitwise_or(subset,thin)\n",
    "    # Keep the cummulative for weighting\n",
    "    weight += thin\n",
    "    # Set the eroded image for next iteration\n",
    "    mask = erode.copy()\n",
    "  \n",
    "  if not return_weight:\n",
    "    return thin\n",
    "  else:\n",
    "    return thin, weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_tree(mask, start):\n",
    "    rows, cols = mask.shape[:2]\n",
    "    stack = [start]\n",
    "    visited = set()\n",
    "    parent = {}\n",
    "    directions = [(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)] \n",
    "    dfs_tree = defaultdict(list)\n",
    "    \n",
    "    while stack:\n",
    "        x, y = stack.pop()\n",
    "\n",
    "        if (x, y) in visited:\n",
    "            continue\n",
    "        \n",
    "        visited.add((x, y))\n",
    "        \n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and mask[nx, ny] == 1 and (nx, ny) not in visited:\n",
    "                stack.append((nx, ny))\n",
    "                parent[(nx, ny)] = (x, y)\n",
    "                dfs_tree[(x, y)].append((nx, ny))\n",
    "    \n",
    "    return dfs_tree, parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_path(tree, start):\n",
    "    def dfs(node, path):\n",
    "        path.append(node)\n",
    "        max_path = path[:]\n",
    "        \n",
    "        for neighbor in tree[node]:\n",
    "            new_path = dfs(neighbor, path[:])\n",
    "            if len(new_path) > len(max_path):\n",
    "                max_path = new_path\n",
    "        \n",
    "        return max_path\n",
    "    \n",
    "    return dfs(start, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_field(logit_map, distance=15, beta=0.5, alpha=0.05):\n",
    "    if isinstance(distance, int):\n",
    "        distance = (distance, distance)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, distance)\n",
    "    dilated_mask = cv2.morphologyEx(cv2.dilate((logit_map > beta).astype(int).astype(np.uint8), kernel, iterations=5), cv2.MORPH_GRADIENT, kernel)\n",
    "    possible = scipy.ndimage.binary_fill_holes(np.where(logit_map > alpha, 1, 0)).astype(int).astype(np.uint8)\n",
    "    dilated_possible = cv2.dilate(possible, kernel, iterations=3)\n",
    "    negative_field = dilated_mask - dilated_mask * dilated_possible\n",
    "    return negative_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sampling(mask, grid=8, alpha=0.1):\n",
    "    if isinstance(grid, int):\n",
    "        grid = (grid, grid)\n",
    "    # We cannot sampling on grid\n",
    "    x, y = np.linspace(0, 1, grid[0])[:-1], np.linspace(0, 1, grid[1])[:-1]\n",
    "    patch_size = np.array(mask.shape[:2]) // grid\n",
    "    mesh = np.floor(np.stack(np.meshgrid(x, y), axis=-1).reshape(-1, 2) * mask.shape[:2]).astype(int)\n",
    "    def sample(src):\n",
    "        dst = src + patch_size\n",
    "        if np.mean(mask[src[0]:dst[0],src[1]:dst[1]]) < alpha:\n",
    "            return [-1, -1]\n",
    "        possible = np.array(np.where(mask[src[0]:dst[0],src[1]:dst[1]] > 0)).T\n",
    "        return possible[np.random.randint(0, high=possible.shape[0])] + src\n",
    "    output = np.apply_along_axis(sample, 1, mesh)\n",
    "    return output[output[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometry Graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = np.array([(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unilateral_dfs_tree(g_mask, inp_mask, start, weight = 1, alpha=0.1, thresh=0.95, context_size=3, dis_map=None):\n",
    "    rows, cols = g_mask.shape[:2]\n",
    "    stack = PriorityQueue()\n",
    "    stack.put((0, start))\n",
    "\n",
    "    # Dijikstra\n",
    "    status = np.zeros(g_mask.shape, dtype=int)\n",
    "\n",
    "    # This to indicate strictly incremental path\n",
    "    g_mask = np.abs(g_mask - 0.001)\n",
    "    cost = np.full_like(g_mask, 1e4)\n",
    "    \n",
    "    px, py = start\n",
    "    cost[px, py] = 0\n",
    "    leaves = set()\n",
    "    border = set()\n",
    "    begin = start\n",
    "    parent = {}\n",
    "    dfs_tree = defaultdict(list)\n",
    "    \n",
    "    while not stack.empty():\n",
    "        \n",
    "        state, (moves, (x, y)) = stack.get()\n",
    "        i = 0\n",
    "        prior = np.mean(moves, axis=0)\n",
    "        accepted_dir = directions[ (directions * prior).sum(axis=-1) >= 0]\n",
    "        for di in directions:\n",
    "            (dx, dy) = di.tolist()\n",
    "            if len(moves) >= context_size: \n",
    "                moves.pop(0)\n",
    "            di_state = moves + [(dx, dy)]\n",
    "            \n",
    "            nx, ny = x + dx, y + dy\n",
    "            \n",
    "            if dis_map is not None:\n",
    "                dis_map[nx, ny] = np.mean(di_state, axis=0)\n",
    "            \n",
    "            if 0 <= nx < rows and 0 <= ny < cols and g_mask[nx, ny] > alpha and status[nx, ny] <= status[x, y]:\n",
    "                # Update on inverse confidence\n",
    "                if cost[nx, ny] > state + (1 - g_mask[nx, ny]) * weight:\n",
    "                    # Set cost as uncertainty gain \n",
    "                    i += 1\n",
    "                    cost[nx, ny] = state + (1 - g_mask[nx, ny]) * weight\n",
    "                    # Start by largest margin\n",
    "                    stack.put((cost[nx, ny], (di_state, (nx, ny))))\n",
    "                    # Erase entry from other branch\n",
    "                    if (nx, ny) in parent.keys():\n",
    "                        # print(dfs_tree[parent[(nx, ny)]])\n",
    "                        dfs_tree[parent[(nx, ny)]].remove((nx, ny))\n",
    "\n",
    "                    parent[(nx, ny)] = (x, y)\n",
    "                    # Determine that they have gone out of beta mask\n",
    "                    # Odd for out-going, Even for in-going.\n",
    "                    if (g_mask[nx, ny] - thresh) * (g_mask[x, y] - thresh) < 0 :\n",
    "                        if status[nx, ny] == 0:\n",
    "                            border.add((nx, ny))\n",
    "                        status[nx, ny] = status[x, y] + 1\n",
    "                    else:\n",
    "                        status[nx, ny] = status[x, y] \n",
    "                    dfs_tree[(x, y)].append((nx, ny))\n",
    "            \n",
    "        if i == 0:\n",
    "            # Force leaves to be sink\n",
    "            status[x, y] += inp_mask[x, y] % 2\n",
    "            leaves.add((x, y))\n",
    "            # Continual of flow\n",
    "            if inp_mask[x, y] == 1:\n",
    "                begin = (x, y)\n",
    "    return {'dfs_tree': dfs_tree, \n",
    "            'parent': parent, \n",
    "            'cost': cost, \n",
    "            'border': border, \n",
    "            'leaves': leaves, \n",
    "            'status': status, \n",
    "            'begin': begin,\n",
    "            'dis_map': dis_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_tree(g_mask, inp_mask, start, weight = 1, alpha=0.1, thresh=0.95):\n",
    "    rows, cols = g_mask.shape[:2]\n",
    "    stack = PriorityQueue()\n",
    "    stack.put((0, start))\n",
    "\n",
    "    # Dijikstra\n",
    "    status = np.zeros(g_mask.shape, dtype=int)\n",
    "\n",
    "    # This to indicate strictly incremental path\n",
    "    g_mask = np.abs(g_mask - 0.001)\n",
    "    cost = np.full_like(g_mask, 1e4)\n",
    "    px, py = start\n",
    "    cost[px, py] = 0\n",
    "    leaves = set()\n",
    "    border = set()\n",
    "    begin = start\n",
    "    parent = {}\n",
    "    directions = [(-1, 0),(-1, -1), (0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1)] \n",
    "    dfs_tree = defaultdict(list)\n",
    "    \n",
    "    while not stack.empty():\n",
    "        \n",
    "        state, (x, y) = stack.get()\n",
    "        i = 0\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and g_mask[nx, ny] > alpha and status[nx, ny] <= status[x, y]:\n",
    "                # Update on inverse confidence\n",
    "                if cost[nx, ny] > state + (1 - g_mask[nx, ny]) * weight:\n",
    "                    # Set cost as uncertainty gain \n",
    "                    i += 1\n",
    "                    cost[nx, ny] = state + (1 - g_mask[nx, ny]) * weight\n",
    "                    # Start by largest margin\n",
    "                    stack.put((cost[nx, ny], (nx, ny)))\n",
    "                    # Erase entry from other branch\n",
    "                    if (nx, ny) in parent.keys():\n",
    "                        # print(dfs_tree[parent[(nx, ny)]])\n",
    "                        dfs_tree[parent[(nx, ny)]].remove((nx, ny))\n",
    "\n",
    "                    parent[(nx, ny)] = (x, y)\n",
    "                    # Determine that they have gone out of mask\n",
    "                    # Odd for out-going, Even for in-going.\n",
    "                    if (g_mask[nx, ny] - thresh) * (g_mask[x, y] - thresh) < 0 :\n",
    "                        if status[nx, ny] == 0:\n",
    "                            border.add((nx, ny))\n",
    "                        status[nx, ny] = status[x, y] + 1\n",
    "                    else:\n",
    "                        status[nx, ny] = status[x, y] \n",
    "                    dfs_tree[(x, y)].append((nx, ny))\n",
    "            \n",
    "        if i == 0:\n",
    "            # Force leaves to be sink\n",
    "            status[x, y] += inp_mask[x, y] % 2\n",
    "            leaves.add((x, y))\n",
    "            # Continual of flow\n",
    "            if inp_mask[x, y] == 1:\n",
    "                begin = (x, y)\n",
    "    return {'dfs_tree': dfs_tree, \n",
    "            'parent': parent, \n",
    "            'cost': cost, \n",
    "            'border': border, \n",
    "            'leaves': leaves, \n",
    "            'status': status, \n",
    "            'begin': begin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do post \n",
    "def longest_path_branching(tree, start):\n",
    "    visited = set()\n",
    "    branches = []\n",
    "    def dfs(node, path, visited, branches):\n",
    "        if len(tree[node]) == 0:\n",
    "            return [node]\n",
    "        if node in visited:\n",
    "            return path\n",
    "        paths = []\n",
    "        visited.add(node)\n",
    "        for neighbor in tree[node]:\n",
    "            if neighbor in visited:\n",
    "                continue\n",
    "            paths.append(dfs(neighbor, path[:], visited, branches) + [node])\n",
    "\n",
    "        if len(paths) == 0:\n",
    "            max_path = [node]\n",
    "        else:\n",
    "            paths = sorted(paths, key= lambda x: -len(x))\n",
    "            max_path = paths[0]\n",
    "            branches += paths[1:]\n",
    "        return max_path\n",
    "    output = dfs(start, [], visited, branches)\n",
    "    return  branches + [output] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, label):\n",
    "    total = label.shape[0] * label.shape[1]\n",
    "    intersection = pred * label\n",
    "    int_count = intersection.sum()\n",
    "    union = cv2.bitwise_or(pred, label)\n",
    "    u_count = union.sum()\n",
    "    dice = 2 * int_count / (int_count + u_count + 1e-6)\n",
    "    iou = int_count / u_count \n",
    "    acc = (pred == label).sum() / total\n",
    "    recall = int_count / pred.sum()\n",
    "    f1 = 2 * acc * recall / (acc + recall)\n",
    "    return {'dice': dice,\n",
    "            'iou': iou,\n",
    "            'acc': acc,\n",
    "            'recall': recall,\n",
    "            'f1': f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/hpc/potato/airc/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/v2/2015.png\"\n",
    "checkpoint = \"/work/hpc/potato/sam/sam2/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "model_id =  \"facebook/sam2.1-hiera-small\"\n",
    "# y, x, h, w = 2400, 0, 1200, 1200\n",
    "# pts = np.array([570, 1660])\n",
    "first_pts = (1900, 2025)\n",
    "patch_size = [512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdownload_ckpts.sh\u001b[0m*         sam2.1_hiera_large.pt  sam2.1_hiera_tiny.pt\n",
      "sam2.1_hiera_base_plus.pt  sam2.1_hiera_small.pt\n"
     ]
    }
   ],
   "source": [
    "%ls /work/hpc/potato/sam/sam2/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A wrapper for Sam inference\n",
    "# class SamInferer:\n",
    "#     def __init__(self, cfg = \"\", \n",
    "#                     ckpt: str = \"\", \n",
    "#                     patch_size = [512, 512],\n",
    "#                     patience = 5,\n",
    "#                     min_dis = 10,\n",
    "#                     alpha=0.1, \n",
    "#                     beta=0.5, \n",
    "#                     post_act=True, \n",
    "#                     min_length=5,\n",
    "#                     kernel_size=3,\n",
    "#                     fill_kernel_size=7,\n",
    "#                     neg_dis=15,\n",
    "#                     sampling_grid=6,\n",
    "#                     thresh=0.75,\n",
    "#                     decay=0.5\n",
    "#                     ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = SamInferer(cfg=model_cfg, \n",
    "                    ckpt=checkpoint,\n",
    "                    roi=[256, 256],\n",
    "                    root_area=1200,\n",
    "                    max_roots=2,\n",
    "                    min_dis=15,\n",
    "                    patience=4,\n",
    "                    beta=0.6,\n",
    "                    alpha=0.2,\n",
    "                    decay=0.3,\n",
    "                    patch_size=patch_size,\n",
    "                    fill_kernel_size=5,\n",
    "                    thresh=0.75,\n",
    "                    min_length=50,\n",
    "                    back_off=5,\n",
    "                    neg_dis=20,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.read(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.add_queue(first_pts, isroot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "(256, 256) (1,) (1, 2)\n",
      "[512 512] (512, 512) float32\n",
      "Score 0.8727840781211853\n",
      "With 1.0 positives and 0.0 negatives\n",
      "{0: 0, np.float32(0.11066823): 0, np.float32(0.98896164): 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (512,512) (0,0) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(param\u001b[38;5;241m.\u001b[39mroi)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[7], line 302\u001b[0m, in \u001b[0;36mSamInferer.iter\u001b[0;34m(self, seg_res, debug)\u001b[0m\n\u001b[1;32m    300\u001b[0m prob_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits[root[\u001b[38;5;241m0\u001b[39m]: dst[\u001b[38;5;241m0\u001b[39m], root[\u001b[38;5;241m1\u001b[39m]:dst[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[root[\u001b[38;5;241m0\u001b[39m]: dst[\u001b[38;5;241m0\u001b[39m], root[\u001b[38;5;241m1\u001b[39m]:dst[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    301\u001b[0m prob_map \u001b[38;5;241m=\u001b[39m sigmoid(prob_map) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_act \u001b[38;5;28;01melse\u001b[39;00m prob_map\n\u001b[0;32m--> 302\u001b[0m confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mgaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Get beta_mask\u001b[39;00m\n\u001b[1;32m    305\u001b[0m quantized_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mpower(prob_map, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (prob_map \u001b[38;5;241m-\u001b[39m beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x, mean, std)\u001b[0m\n\u001b[1;32m      1\u001b[0m sigmoid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[0;32m----> 2\u001b[0m gaussian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x,mean,std: np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2.5\u001b[39m \u001b[38;5;241m*\u001b[39m std)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (512,512) (0,0) "
     ]
    }
   ],
   "source": [
    "# plt.imshow(skimage.morphology.skeletonize(output['possible']))\n",
    "iterations += 1\n",
    "print(f\"Iteration {iterations}\")\n",
    "output = param.iter(debug=True)\n",
    "print(param.roi)\n",
    "if output['ret'] is True:\n",
    "    src, dst = param.roi[:2], param.roi[2:]\n",
    "    image = param.image[src[0]:dst[0], src[1]:dst[1], ::-1]\n",
    "    # logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "    logit = output['prob_map']\n",
    "    p_mask = param.b_mask[src[0]:dst[0], src[1]:dst[1]].copy()\n",
    "    # p_mask = (p_mask >= np.quantile(p_mask[p_mask > 0], 0.5)).astype(np.uint8)\n",
    "    # Plotting\n",
    "    plot = image[:, :, ::-1] * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "    plot = plot * (1 - p_mask[..., None])\n",
    "    # plot = plot.astype(np.uint8)\n",
    "    # plt.imshow(plot)\n",
    "    # plt.colorbar()\n",
    "    # \n",
    "    cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(int).astype(np.uint8))\n",
    "    branches = [np.array(branch) for branch in output['branches']]\n",
    "    annotation = output['infer']['pts'] + (param.root - param.roi[:2])[::-1]\n",
    "    a_label = output['infer']['label'][:, None]\n",
    "    color = [0, 1, 0, 0.5] * a_label + [1, 0, 0, 0.5] * (1 - a_label)\n",
    "    # plt.scatter(annotation[:, 0], annotation[:, 1], s=50, c=color, marker='*')\n",
    "    # plot = image.astype(float) / 255\n",
    "    for pt, c in zip(annotation, color):\n",
    "        cv2.circle(plot, pt, 5, c, -1)\n",
    "    cv2.circle(plot, (np.array(output['infer']['prompt']) - src)[::-1], 10, [1, 1, 0, 0.5], -1)\n",
    "    # plt.title(f\"Score: {output['infer']['score']}\")\n",
    "    # pt = output['infer']['prompt']\n",
    "    # plt.scatter(pt[1], pt[0])\n",
    "    cmap = plt.get_cmap('hsv')\n",
    "    for i, branch in enumerate(branches):\n",
    "        c_val = cmap(float(i) / len(branches))\n",
    "        # print(c_val)\n",
    "        # rgb = (int(c_val[0] * 255), int(c_val[1] * 255), int(c_val[2] * 255))\n",
    "        cv2.polylines(plot, [branch[:, ::-1]], False, c_val, 2)\n",
    "    # \"|\n",
    "    cv2.imwrite(f\"/work/hpc/potato/airc/data/viz/iteration_{iterations}_v4.jpg\", (plot * 255)[..., ::-1])\n",
    "    plt.imshow(plot)\n",
    "    plt.title(f\"Metrics: {output['metrics']}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "else:\n",
    "    print(\"Hehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.threshold((logit * 255).astype(int).astype(np.uint8), 20, 1, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1])                                          `\n",
    "# plt.colorbar()\n",
    "gain = np.sqrt(np.pi)\n",
    "std = self.beta.mean() * param.w_kernel.mean() / scipy.ndimage.gaussian_filter(param.weight[src[0]:dst[0], src[1]:dst[1]], 30)\n",
    "dirac = lambda x, mean, std:  np.exp((x - mean) / (std * 1.44)) / (2.5 * std)\n",
    "beta = param.beta[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "\n",
    "p_logit = skimage.restoration.denoise_wavelet(logit[..., None], channel_axis=-1, rescale_sigma=True)\n",
    "softmask = dirac(logit ** 1.5, beta, std) * (logit > param.alpha)\n",
    "s = np.interp(softmask, (softmask.min(), softmask.max()), (0, 1))\n",
    "pseudo_mask = (s > 0.4).astype(np.uint8)\n",
    "cand = np.minimum(s * 255, 255).astype(np.uint8)\n",
    "# softmask = sigmoid(cand)\n",
    "\n",
    "# softmask[softmask > 255] = 255\n",
    "mask = cv2.adaptiveThreshold(cand, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, -5) * (softmask > param.alpha)\n",
    "# most_confidence =\n",
    "# mask = cv2.threshold(cand.astype(np.uint8), 200, 1, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "# radius = 15\n",
    "# footprint = skimage.morphology.disk(radius)\n",
    "# local_otsu = skimage.filters.rank.otsu(softmask.astype(np.uint8), footprint)\n",
    "# mask = (softmask > local_otsu).astype(np.uint8)\n",
    "# mask = prune(mask, min_size=50)\n",
    "mask = np.maximum(mask, pseudo_mask)\n",
    "mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones([3, 3], dtype=np.uint8), 2)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=[8, 6])\n",
    "im = ax1.imshow(image[..., ::-1])\n",
    "ax1.set_title(\"Original Logit\")\n",
    "fig.colorbar(im, ax=ax1)\n",
    "\n",
    "im=ax2.imshow(s)\n",
    "ax2.set_title(\"Softmask\")\n",
    "fig.colorbar(im, ax=ax2)\n",
    "\n",
    "im=ax3.imshow(pseudo_mask)\n",
    "ax3.set_title(\"Adaptive Thresholding\")\n",
    "fig.colorbar(im, ax=ax3)\n",
    "\n",
    "ax4.imshow(mask)\n",
    "ax4.set_title(\"Beta Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(softmask)\n",
    "pos = np.unravel_index(index, softmask.shape, order='F')\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = output['infer']['pts']\n",
    "plt.imshow(output['infer']['input'])\n",
    "plt.axis('off')\n",
    "# plt.scatter(pts[:, 0], pts[:, 1], marker='s', s=5, color=(1, 0, 1))\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['infer']['negative'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = param.root\n",
    "plt.imshow(param.image[x:x+512, y:y+512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts, _ = cv2.findContours(p_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "output = cv2.drawContours(image.copy(), cnts, -1, (0,0,255), 3)\n",
    "plt.imshow(output[..., ::-1])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = param.label[src[0]:dst[0], src[1]:dst[1], None] \n",
    "pred = param.b_mask[src[0]:dst[0], src[1]:dst[1], None]\n",
    "# Red for FP, Blue for FN\n",
    "# fp = pred * (1 - mask) * np.array([0, 0, .2])[None, None, :]\n",
    "# fn = mask * (1 - pred)  * np.array([.2, 0, 0])[None, None, :]\n",
    "# masks = pred\n",
    "# save_img = image * (1 - masks) + 255 * masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred.shape, image.shape)\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/image.jpg\", param.image[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/seg_image.jpg\", image)\n",
    "# cv2.imwrite(\"/work/hpc/potato/airc/data/viz/seg_pred.jpg\", pred * 255)\n",
    "# cv2.imwrite(\"/work/hpc/potato/airc/data/viz/seg_label.jpg\", mask * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param.b_mask[src[0]:dst[0], src[1]:dst[1]] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_image = cv2.cvtColor(param.image, cv2.COLOR_BGR2HSV)\n",
    "unsharp = cv2.bilateralFilter(hsv_image,9,50,75)\n",
    "hsv_image = 2 * hsv_image - unsharp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = logit > 0.2\n",
    "# kernel = np.ones([3, 3], dtype=np.uint8)\n",
    "# skeleton = skimage.morphology.skeletonize(param.label)\n",
    "# plt.imshow(param.image[..., ::-1])\n",
    "# plt.imshow(image[..., ::-1])\n",
    "plt.imshow(hsv_image[src[0]:dst[0], src[1]:dst[1], 2])\n",
    "# keypoints = np.argmax(logit, axis=1)\n",
    "# print(keypoints.shape,  logit.shape, logit.max())\n",
    "# pts = [logit[i, pt] for i, pt in enumerate(keypoints)]\n",
    "# chosen = np.argmax(pts)\n",
    "# pt = keypoints[chosen]\n",
    "# print((chosen, pt) + src, logit[chosen, pt], src)\n",
    "# plt.colorbar()\n",
    "# plt.imshow(hsv_image[src[0]:dst[0], src[1]:dst[1], 0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.graph_root = (453, 1488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "plt.imshow(bgr_image[src[0]:dst[0], src[1]:dst[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = param.logits / param.weight\n",
    "param.b_mask = (logit ** 1.5 > param.beta).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = torch.from_numpy(param.a_mask[src[0]:dst[0], src[1]:dst[1]].copy()).float().cuda()\n",
    "x = torch.from_numpy(logit).float().cuda()\n",
    "canvas = torch.from_numpy(output['canvas']).float().cuda()\n",
    "canvas /= canvas.max()\n",
    "dist = geo.GSF2d(canvas[None, None, :], x[None, None, :], theta=1., v=8, lamb=.8, iter = 4).cpu().numpy()[0, 0]\n",
    "# plt.imshow(dist > np.quantile(dist[dist > 0], 0.2))\n",
    "plt.imshow(dist)\n",
    "plt.colorbar()\n",
    "plt.title(f\"{ np.quantile(dist[dist > dist.min()], 0.2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(skimage.morphology.skeletonize(output['possible']))\n",
    "\n",
    "src, dst = param.root, param.root + param.patch_size\n",
    "image = param.image[src[0]:dst[0], src[1]:dst[1]]\n",
    "logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "# Plotting\n",
    "plot = image[:, :, ::-1] * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "plot = plot * (1 - param.b_mask[src[0]:dst[0], src[1]:dst[1]][..., None])\n",
    "plt.imshow(plot)\n",
    "# plt.colorbar()\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(int).astype(np.uint8))\n",
    "branches = [np.array(branch) for branch in output['branches']]\n",
    "prompt = output['infer']['prompt'] - param.root\n",
    "annotation = output['infer']['pts']\n",
    "a_label = output['infer']['label'][:, None]\n",
    "color = [0, 0, 1, 0.5] * a_label + [1, 0, 0, 0.5] * (1 - a_label)\n",
    "plt.title(f\"Score: {output['infer']['score']}\")\n",
    "plt.scatter(annotation[:, 0], annotation[:, 1], s=50, c=color, marker='*')\n",
    "plt.scatter(prompt[1], prompt[0], s=100, c=[1, 1, 0, 1], marker='X')\n",
    "# pt = output['infer']['prompt']\n",
    "# plt.scatter(pt[1], pt[0])\n",
    "for branch in branches:\n",
    "    plt.plot(branch[:, 1], branch[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.98\n",
    "score = np.quantile(prob_map, p)\n",
    "prob_map = sigmoid(output['infer']['logit'])\n",
    "plt.imshow(prob_map > score)\n",
    "plt.title(f\"{p}-quantile {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = param.pos[:, None, :] - param.pos[None, :, :]\n",
    "loss = np.linalg.norm(dist, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit ** 2)\n",
    "plt.title(f\"Score {output['infer']['score']}\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit > 0.458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(logit > param.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logit.min(), logit.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = logit ** 0.3\n",
    "plt.imshow(image[:, :, ::-1] * (1- ret[..., None]) / 255 + ret[..., None])\n",
    "\n",
    "plt.scatter(pt[1], pt[0], s=0.5, c=(1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = param.iter()\n",
    "src, dst = param.root, param.root + param.patch_size\n",
    "image = param.image[src[0]:dst[0], src[1]:dst[1]]\n",
    "logit = param.logits[src[0]:dst[0], src[1]:dst[1]] / param.weight[src[0]:dst[0], src[1]:dst[1]]\n",
    "# Plotting\n",
    "plot = image[:, :, ::-1] * (1- logit[..., None]) / 255 + logit[..., None]\n",
    "plot = plot * (1 - param.b_mask[src[0]:dst[0], src[1]:dst[1]][..., None])\n",
    "plt.imshow(plot)\n",
    "# plt.colorbar()\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", (np.repeat(output['thin'][:, :, None] / output['thin'].max(), 3, axis=-1) * 255).astype(int).astype(np.uint8))\n",
    "branches = [np.array(branch) for branch in output['branches']]\n",
    "pt = output['infer']['prompt'] - param.root\n",
    "plt.scatter(pt[1], pt[0])\n",
    "for branch in branches: \n",
    "    print(branch.shape[0])\n",
    "    plt.plot(branch[:, 1], branch[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)\n",
    "plt.title(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sigmoid(logit)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "possible = cv2.morphologyEx(np.where(prob > param.alpha, 1, 0).astype(int).astype(np.uint8), cv2.MORPH_CLOSE, kernel, 1)\n",
    "possible = scipy.ndimage.binary_fill_holes(possible)\n",
    "possible = getLargestCC(possible).astype(int).astype(np.uint8)\n",
    "plt.imshow(possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin, thin_w = morpholgy_thinning(possible, return_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thin_w * (thin_w < thin_w.max() * 0.75))\n",
    "plt.title(f\"Max:{thin_w.max()} \")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "stable = ((thin_w > 0) & (thin_w < thin_w.max() * 0.75)).astype(int).astype(np.uint8)\n",
    "closed = cv2.morphologyEx(stable, cv2.MORPH_DILATE, kernel, 1)\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/closed.jpg\", np.repeat(closed[:, :, None], 3, axis=-1) * 255)\n",
    "plt.imshow(closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = skimage.morphology.skeletonize(closed)\n",
    "plt.imshow(skeleton)\n",
    "main_branch = getLargestCC(skeleton).astype(int).astype(np.uint8)\n",
    "plt.imshow(main_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = (main_branch * prob) / score\n",
    "w_map[mask * main_branch == 1] = 1\n",
    "cv2.imwrite(\"/work/hpc/potato/airc/data/viz/output.jpg\", (np.repeat(w_map[:, :, None], 3, axis=2) * 255).astype(int).astype(np.uint8))\n",
    "plt.imshow(w_map)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit map is same shape to mask prompt, which is half the size of image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start at stable mask\n",
    "w_pts = np.array(np.where(w_map == 1)).T\n",
    "print(w_pts.shape)\n",
    "dist = np.linalg.norm(w_pts - res['pts'][-1] / 2, axis=1)\n",
    "nn = w_pts[np.argmin(dist)]\n",
    "print(nn, res['pts'][-1] / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_res = dfs_tree(w_map.copy(), res['inp_mask'], tuple(nn), alpha=0.01, thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, branches = longest_path_branching(dfs_res['dfs_tree'], tuple(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.array(path)\n",
    "branches = [np.array(branch) for branch in branches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = np.array(list(dfs_res['leaves']))\n",
    "cost_map = dfs_res['cost']\n",
    "cost_map[cost_map > 100] = 0\n",
    "plt.plot(path[:, 1], path[:, 0])\n",
    "for branch in branches: \n",
    "    plt.plot(branch[:, 1], branch[:, 0])\n",
    "plt.imshow(cost_map ** 0.3)\n",
    "plt.scatter(leaves[:, 1], leaves[:, 0], s=0.2)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_branch = [branches[i] for i in range(len(branches)) if len(branches[i]) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accepted_branch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
